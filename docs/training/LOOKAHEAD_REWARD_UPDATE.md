# ğŸ¯ Lookahead Reward System æ›´æ–°è¯´æ˜

## ğŸ“‹ ä¿®æ”¹æ€»ç»“

æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œä¿®æ”¹äº†MCTSçš„rewardè¯„ä¼°ç³»ç»Ÿï¼š

**ç”¨æˆ·éœ€æ±‚ï¼š**
> "mctsé‡Œé¢å¥½åçš„æ‰“åˆ†ç”¨è¿™æ¬¡actioné€ æˆçš„åŠ åˆ†å’Œæ­¤åx=10ä¸ªçƒçš„æ€»åŠ åˆ†ä½œä¸ºè¯„ä»·è¿™ä¸ªactionå¾—åˆ†å¥½åçš„æ ‡å‡†ï¼›æ¸¸æˆç»“æŸä¸ç”¨æ‰£åˆ†"

**å®ç°æ–¹æ¡ˆï¼š**
1. âœ… æ·»åŠ 10æ­¥lookaheadæ¨¡æ‹Ÿè®¡ç®—æœªæ¥å¥–åŠ±
2. âœ… ç§»é™¤æ¸¸æˆç»“æŸçš„death penalty (-1.0)
3. âœ… ç»“åˆimmediate reward + lookahead rewardä½œä¸ºç»¼åˆè¯„ä¼°

---

## ğŸ”§ ä»£ç ä¿®æ”¹è¯¦æƒ…

### 1. SimplifiedGameState.simulate_lookahead()

**æ–‡ä»¶ï¼š** `mcts/MCTS.py:154-205`

**æ–°å¢æ–¹æ³•ï¼š**
```python
def simulate_lookahead(self, num_steps: int = 10, policy: str = "greedy") -> float:
    """
    æ¨¡æ‹Ÿæœªæ¥Nä¸ªæ°´æœçš„æ”¾ç½®ï¼Œè¿”å›æ€»å¾—åˆ†

    Args:
        num_steps: æ¨¡æ‹Ÿæ­¥æ•° (é»˜è®¤10)
        policy: "greedy" (è´ªå¿ƒ) æˆ– "random" (éšæœº)

    Returns:
        lookaheadæœŸé—´è·å¾—çš„æ€»åˆ†æ•°
    """
```

**å·¥ä½œåŸç†ï¼š**
1. åˆ›å»ºå½“å‰çŠ¶æ€çš„æ·±æ‹·è´
2. å¾ªç¯Næ¬¡ï¼š
   - è·å–æœ‰æ•ˆåŠ¨ä½œ
   - æ ¹æ®policyé€‰æ‹©åŠ¨ä½œ (è´ªå¿ƒ/éšæœº)
   - æ‰§è¡ŒåŠ¨ä½œå¹¶ç´¯ç§¯åˆ†æ•°
   - æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ
3. è¿”å›ï¼š`æ¨¡æ‹Ÿååˆ†æ•° - åˆå§‹åˆ†æ•°`

**æµ‹è¯•ç»“æœï¼š**
```
Initial score: 0
10-step greedy lookahead: 10.00
10-step random lookahead: 12.00
Original state unchanged: 0 âœ…
```

---

### 2. AlphaZeroMCTS.evaluate_with_lookahead()

**æ–‡ä»¶ï¼š** `AlphaZeroMCTS.py:224-261`

**æ–°å¢æ–¹æ³•ï¼š**
```python
def evaluate_with_lookahead(self, state, parent_score=0.0) -> float:
    """
    ç»“åˆç½‘ç»œé¢„æµ‹å’Œlookaheadæ¨¡æ‹Ÿçš„ç»¼åˆè¯„ä¼°

    Returns:
        blended_value = 70% lookahead + 30% network
    """
```

**è¯„ä¼°æµç¨‹ï¼š**
```
1. ç½‘ç»œä»·å€¼é¢„æµ‹ â†’ network_value
2. å³æ—¶å¥–åŠ± = state.score - parent_score
3. Lookaheadå¥–åŠ± = simulate_lookahead(10æ­¥)
4. æ€»å¥–åŠ± = å³æ—¶ + lookahead
5. å½’ä¸€åŒ– â†’ [-1, 1]
6. æ··åˆ = 0.7 * lookahead + 0.3 * network
```

**ä¸ºä»€ä¹ˆæ··åˆï¼Ÿ**
- 70% lookaheadï¼šåˆ©ç”¨å®é™…æ¨¡æ‹Ÿç»“æœï¼Œæ›´å‡†ç¡®
- 30% networkï¼šä¿æŒç½‘ç»œå­¦ä¹ èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡åº¦ä¾èµ–æ¨¡æ‹Ÿ

---

### 3. ç§»é™¤Death Penalty

**æ–‡ä»¶ï¼š** `AlphaZeroMCTS.py:274-278`

**ä¿®æ”¹å‰ï¼š**
```python
if node.state.is_terminal:
    value = -1.0  # âŒ æ‰€æœ‰ç»ˆå±€éƒ½æ˜¯è´Ÿä»·å€¼
```

**ä¿®æ”¹åï¼š**
```python
if node.state.is_terminal:
    # ä½¿ç”¨å½’ä¸€åŒ–åˆ†æ•°ä»£æ›¿å›ºå®šæƒ©ç½š
    normalized_score = node.state.score / 500.0
    value = float(np.clip(normalized_score - 0.5, -1.0, 1.0))
```

**æ•ˆæœå¯¹æ¯”ï¼š**
| ç»ˆå±€åˆ†æ•° | æ—§value | æ–°value |
|---------|---------|---------|
| 50åˆ†    | -1.0    | -0.40   |
| 120åˆ†   | -1.0    | -0.26   |
| 250åˆ†   | -1.0    | 0.00    |
| 400åˆ†   | -1.0    | 0.30    |

ç°åœ¨é«˜åˆ†ç»“æŸä¸å†è¢«æƒ©ç½šï¼

---

### 4. MCTS Searché›†æˆ

**æ–‡ä»¶ï¼š** `AlphaZeroMCTS.py:327-334`

**ä¿®æ”¹ï¼š**
```python
# è·å–çˆ¶èŠ‚ç‚¹åˆ†æ•°
parent_score = node.parent.state.score if node.parent else 0.0

# ä½¿ç”¨lookaheadè¯„ä¼°
value = self.evaluate_with_lookahead(node.state, parent_score)
```

æ¯æ¬¡MCTSæ¨¡æ‹Ÿéƒ½ä¼šï¼š
1. è®¡ç®—immediate reward (å½“å‰-çˆ¶èŠ‚ç‚¹åˆ†æ•°)
2. è¿è¡Œ10æ­¥greedy lookahead
3. ç»¼åˆè¯„ä¼°ä½œä¸ºèŠ‚ç‚¹ä»·å€¼

---

## ğŸ“Š æ–°å‚æ•°

### AlphaZeroMCTSæ„é€ å‡½æ•°æ–°å¢å‚æ•°ï¼š

```python
AlphaZeroMCTS(
    network=...,
    # ... åŸæœ‰å‚æ•° ...
    use_lookahead=True,      # æ˜¯å¦ä½¿ç”¨lookahead (é»˜è®¤True)
    lookahead_steps=10       # lookaheadæ­¥æ•° (é»˜è®¤10)
)
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# æ ‡å‡†é…ç½® (10æ­¥lookahead)
mcts = AlphaZeroMCTS(network, use_lookahead=True, lookahead_steps=10)

# æ›´é•¿lookahead (æ›´å‡†ç¡®ä½†æ›´æ…¢)
mcts = AlphaZeroMCTS(network, use_lookahead=True, lookahead_steps=15)

# å…³é—­lookahead (ä»…ç”¨ç½‘ç»œè¯„ä¼°)
mcts = AlphaZeroMCTS(network, use_lookahead=False)
```

---

## ğŸ§ª æµ‹è¯•ç»“æœ

è¿è¡Œ `python test_lookahead_reward.py`:

```
âœ… Test 1: Lookahead Simulation - PASSED
  - Greedy policy æ­£å¸¸å·¥ä½œ
  - Random policy æ­£å¸¸å·¥ä½œ
  - åŸçŠ¶æ€æœªè¢«ä¿®æ”¹

âœ… Test 2: MCTS with Lookahead - PASSED
  - ä½¿ç”¨lookaheadçš„MCTSé€‰æ‹©ä¸åŒåŠ¨ä½œ
  - ä¸ä½¿ç”¨lookaheadçš„MCTSä¹Ÿæ­£å¸¸

âœ… Test 3: Terminal Value (No Death Penalty) - PASSED
  - 120åˆ†ç»ˆå±€: -0.26 (æ—§: -1.0)
  - æ­£ç¡®å½’ä¸€åŒ–

âœ… Test 4: Lookahead Policy Comparison - PASSED
  - 5æ­¥: 24åˆ†
  - 10æ­¥: 24åˆ†
  - 15æ­¥: 19åˆ†
```

---

## ğŸ”„ å®Œæ•´æ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCTS Simulation                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Selection (PUCT)                      â”‚
â”‚    é€‰æ‹©æœ€ä¼˜å­èŠ‚ç‚¹                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Evaluation                            â”‚
â”‚    â”œâ”€ ç½‘ç»œé¢„æµ‹: policy, value_net       â”‚
â”‚    â”œâ”€ Immediate Reward: score - parent  â”‚
â”‚    â”œâ”€ Lookahead: simulate 10 steps     â”‚
â”‚    â””â”€ Blended: 0.7*lookahead + 0.3*net â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Expansion                             â”‚
â”‚    åˆ›å»ºå­èŠ‚ç‚¹ (ç”¨ç½‘ç»œpolicyä½œä¸ºprior)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Backup                                â”‚
â”‚    å›ä¼ blended valueåˆ°è·¯å¾„æ‰€æœ‰èŠ‚ç‚¹       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ æ€§èƒ½å½±å“

### æ—¶é—´å¤æ‚åº¦ï¼š
- **åŸMCTS:** O(simulations Ã— network_forward)
- **æ–°MCTS:** O(simulations Ã— (network_forward + lookahead_sims))

å…¶ä¸­ `lookahead_sims = lookahead_steps Ã— avg_valid_actions`

### é¢„ä¼°é€Ÿåº¦å½±å“ï¼š
- 10æ­¥greedy lookahead: æ¯æ­¥å°è¯•~10ä¸ªåŠ¨ä½œ
- æ€»å¼€é”€: ~100æ¬¡çŠ¶æ€å¤åˆ¶ + apply_action
- **é¢„è®¡å‡é€Ÿ: 2-3å€** (å¯æ¥å—ï¼Œå› ä¸ºè¯„ä¼°æ›´å‡†ç¡®)

### ä¼˜åŒ–å»ºè®®ï¼š
1. è°ƒæ•´ `lookahead_steps` (5æ­¥æ›´å¿«ï¼Œ15æ­¥æ›´å‡†)
2. ä½¿ç”¨random policyä»£æ›¿greedy (æ›´å¿«ä½†ä¸å¤Ÿå‡†)
3. åªåœ¨æ ¹èŠ‚ç‚¹é™„è¿‘ä½¿ç”¨lookahead

---

## ğŸš€ è®­ç»ƒå»ºè®®

### æ¨èé…ç½®ï¼š

```python
# TrainAlphaZero.py æˆ– SelfPlay.py
mcts = AlphaZeroMCTS(
    network=network,
    num_simulations=100,        # å¯ä»¥å‡å°‘ (å› ä¸ºlookaheadæä¾›æ›´å¤šä¿¡æ¯)
    c_puct=1.5,
    temperature=1.0,            # è®­ç»ƒæ—¶
    use_lookahead=True,
    lookahead_steps=10,         # 10æ­¥å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦
    add_dirichlet_noise=True    # è®­ç»ƒæ—¶ä¿æŒæ¢ç´¢
)
```

### è®­ç»ƒç›‘æ§é‡ç‚¹ï¼š

1. **Value Losså˜åŒ–**
   - ä¹‹å‰: 0.0015-0.0037 (å¤ªå°)
   - æœŸæœ›: 0.01-0.05 (æ­£å¸¸å­¦ä¹ )

2. **Scoreæå‡**
   - ä¹‹å‰: 84-109 (ä¸ç¨³å®š)
   - æœŸæœ›: æŒç»­ä¸Šå‡è¶‹åŠ¿

3. **è®­ç»ƒæ—¶é—´**
   - æ¯å±€æ¸¸æˆå¯èƒ½å¢åŠ 2-3å€æ—¶é—´
   - ä½†æ ·æœ¬è´¨é‡æ›´é«˜ï¼Œå¯èƒ½éœ€è¦æ›´å°‘å±€æ•°

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›

### 1. æ›´å¥½çš„åŠ¨ä½œè¯„ä¼°
- è€ƒè™‘æœªæ¥10æ­¥è€Œéä»…å½“å‰æ­¥
- è´ªå¿ƒlookaheadæ‰¾åˆ°å±€éƒ¨æœ€ä¼˜è·¯å¾„

### 2. æ›´åˆç†çš„ç»ˆå±€è¯„ä¼°
- é«˜åˆ†ç»“æŸè·å¾—æ­£ä»·å€¼
- ä½åˆ†ç»“æŸæ‰æ˜¯è´Ÿä»·å€¼
- ç½‘ç»œèƒ½å­¦ä¹ åˆ°åˆ†æ•°-ä»·å€¼çš„çœŸå®å…³ç³»

### 3. è®­ç»ƒç¨³å®šæ€§
- Value lossåº”è¯¥å¢å¤§ (è¯´æ˜åœ¨å­¦ä¹ )
- Scoreåº”è¯¥æ›´ç¨³å®šåœ°æå‡
- ç­–ç•¥åº”è¯¥æ›´å€¾å‘é•¿æœŸè§„åˆ’

---

## ğŸ” åç»­ä¼˜åŒ–æ–¹å‘

### 1. è‡ªé€‚åº”lookaheadæ­¥æ•°
```python
# æ ¹æ®æ¸¸æˆé˜¶æ®µè°ƒæ•´
if state.score < 100:
    lookahead_steps = 5   # æ—©æœŸå¿«é€Ÿ
else:
    lookahead_steps = 15  # åæœŸè°¨æ…
```

### 2. ä»·å€¼ç½‘ç»œå¼•å¯¼çš„lookahead
```python
# ç”¨ç½‘ç»œé€‰æ‹©lookaheadä¸­çš„åŠ¨ä½œ
def lookahead_with_network(state):
    for step in range(10):
        policy = network.predict(state)
        action = sample_from(policy)  # è€Œégreedy
        state.apply(action)
```

### 3. å¹¶è¡ŒåŒ–lookahead
- å¤šè¿›ç¨‹åŒæ—¶æ¨¡æ‹Ÿä¸åŒåŠ¨ä½œ
- æ˜¾è‘—æå‡é€Ÿåº¦

---

## ğŸ“ æ–‡ä»¶ä¿®æ”¹æ¸…å•

### ä¿®æ”¹çš„æ–‡ä»¶ï¼š
1. âœ… `mcts/MCTS.py`
   - æ·»åŠ  `simulate_lookahead()` æ–¹æ³•

2. âœ… `AlphaZeroMCTS.py`
   - æ·»åŠ  `use_lookahead`, `lookahead_steps` å‚æ•°
   - æ·»åŠ  `evaluate_with_lookahead()` æ–¹æ³•
   - ä¿®æ”¹ `search()` ä¸­çš„ç»ˆå±€ä»·å€¼è®¡ç®—
   - ä¿®æ”¹ `search()` ä½¿ç”¨lookaheadè¯„ä¼°

### æ–°å¢çš„æ–‡ä»¶ï¼š
3. âœ… `test_lookahead_reward.py`
   - å®Œæ•´çš„æµ‹è¯•å¥—ä»¶
   - 4ä¸ªæµ‹è¯•ç”¨ä¾‹å…¨éƒ¨é€šè¿‡

4. âœ… `LOOKAHEAD_REWARD_UPDATE.md` (æœ¬æ–‡æ¡£)
   - è¯¦ç»†è¯´æ˜æ‰€æœ‰ä¿®æ”¹

### æ— éœ€ä¿®æ”¹çš„æ–‡ä»¶ï¼š
- `TrainAlphaZero.py` - è‡ªåŠ¨ä½¿ç”¨æ–°MCTS
- `SelfPlay.py` - è‡ªåŠ¨ä½¿ç”¨æ–°è¯„ä¼°
- `SuikaNet.py` - ç½‘ç»œç»“æ„ä¸å˜
- `StateConverter.py` - çŠ¶æ€è½¬æ¢ä¸å˜

---

## âœ… éªŒè¯æ¸…å•

è®­ç»ƒå‰è¯·ç¡®è®¤ï¼š

- [x] `test_lookahead_reward.py` æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [x] `simulate_lookahead()` ä¸ä¿®æ”¹åŸçŠ¶æ€
- [x] Terminal value ä½¿ç”¨å½’ä¸€åŒ–åˆ†æ•°
- [x] MCTSæ­£ç¡®è°ƒç”¨ `evaluate_with_lookahead()`
- [x] Lookaheadä½¿ç”¨greedy policy
- [x] æ··åˆæ¯”ä¾‹: 70% lookahead + 30% network

---

**ä¿®æ”¹å®Œæˆæ—¶é—´:** 2025-11-24

**æµ‹è¯•çŠ¶æ€:** âœ… å…¨éƒ¨é€šè¿‡

**è®­ç»ƒçŠ¶æ€:** ğŸš€ å·²å¯åŠ¨

æœŸå¾…æ–°rewardç³»ç»Ÿå¸¦æ¥çš„æ€§èƒ½æå‡ï¼
