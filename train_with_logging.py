"""
å¸¦è¯¦ç»†æ—¥å¿—è®°å½•çš„DQNè®­ç»ƒè„šæœ¬
è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŒ‡æ ‡ï¼Œç”¨äºåç»­å¯è§†åŒ–åˆ†æ
"""
import os
import json
import numpy as np
import paddle
from datetime import datetime
from DQN import (
    Agent, build_model, ReplayMemory, MEMORY_SIZE, MEMORY_WARMUP_SIZE,
    BATCH_SIZE, LEARNING_RATE, GAMMA, set_global_seed
)
from GameInterface import GameInterface
from PRNG import PRNG

# è®­ç»ƒé…ç½®
TRAINING_SEED = 42
MAX_EPISODES = 500  # å¿«é€Ÿæ¼”ç¤ºï¼š500è½®
EVAL_INTERVAL = 25  # æ¯25è½®è¯„ä¼°ä¸€æ¬¡
EVAL_EPISODES = 10  # æ¯æ¬¡è¯„ä¼°10å±€
LOG_FILE = "training_metrics.json"
MODEL_SAVE_DIR = "weights"

# éªŒè¯é›†å’Œæµ‹è¯•é›†çš„å›ºå®šç§å­
VAL_SEEDS = list(range(10000, 10000 + EVAL_EPISODES))
TEST_SEEDS = list(range(20000, 20000 + EVAL_EPISODES))


def evaluate_agent(env, agent, seeds, num_episodes=None):
    """
    åœ¨å›ºå®šç§å­ä¸Šè¯„ä¼°agent

    Args:
        env: æ¸¸æˆç¯å¢ƒ
        agent: è¦è¯„ä¼°çš„agent
        seeds: è¯„ä¼°ä½¿ç”¨çš„ç§å­åˆ—è¡¨
        num_episodes: è¯„ä¼°å±€æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨seedsçš„é•¿åº¦ï¼‰

    Returns:
        dict: åŒ…å«å¹³å‡åˆ†æ•°ã€å¥–åŠ±ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    if num_episodes is None:
        num_episodes = len(seeds)

    scores = []
    rewards = []

    for i in range(num_episodes):
        seed = seeds[i]
        env.reset(seed=seed)

        action = np.random.randint(0, env.action_num)
        feature, _, alive = env.next(action)

        episode_reward = 0
        while alive:
            action = agent.predict(feature)
            feature, reward, alive = env.next(action)
            episode_reward += np.sum(reward)

        scores.append(env.game.score)
        rewards.append(episode_reward)

    return {
        'mean_score': float(np.mean(scores)),
        'std_score': float(np.std(scores)),
        'mean_reward': float(np.mean(rewards)),
        'std_reward': float(np.std(rewards)),
        'max_score': int(np.max(scores)),
        'min_score': int(np.min(scores)),
    }


def run_training_episode(env, agent, memory):
    """
    è¿è¡Œä¸€ä¸ªè®­ç»ƒepisode

    Returns:
        dict: åŒ…å«è¯¥episodeçš„å„ç§æŒ‡æ ‡
    """
    env.reset()

    step = 0
    rewards_sum = 0
    losses = []

    action = np.random.randint(0, env.action_num)
    feature, _, alive = env.next(action)

    while alive:
        step += 1

        action = agent.sample(feature)
        next_feature, reward, alive = env.next(action)

        reward = reward if alive else -1000
        memory.append((feature, action, reward, next_feature, alive))

        # å­¦ä¹ 
        if len(memory) >= MEMORY_WARMUP_SIZE and agent.global_step % 1 == 0:
            batch = memory.sample(BATCH_SIZE)
            loss = agent.learn(*batch)
            losses.append(loss)

        rewards_sum += np.sum(reward)
        feature = next_feature
        agent.global_step += 1

    return {
        'score': int(env.game.score),
        'reward': float(rewards_sum),
        'steps': int(step),
        'mean_loss': float(np.mean(losses)) if losses else 0.0,
        'epsilon': float(agent.e_greed)
    }


def main():
    print("=" * 70)
    print("DQNè®­ç»ƒ - å¸¦è¯¦ç»†æŒ‡æ ‡è®°å½•")
    print("=" * 70)

    # è®¾ç½®éšæœºç§å­
    set_global_seed(TRAINING_SEED)

    # åˆå§‹åŒ–ç¯å¢ƒå’Œagent
    env = GameInterface()
    feature_dim = env.FEATURE_MAP_HEIGHT * env.FEATURE_MAP_WIDTH * 2
    action_dim = env.ACTION_NUM

    memory = ReplayMemory(MEMORY_SIZE)
    agent = Agent(
        build_model,
        feature_dim,
        action_dim,
        e_greed=0.5,
        e_greed_decrement=1e-6
    )

    print(f"ç‰¹å¾ç»´åº¦: {feature_dim}")
    print(f"åŠ¨ä½œç©ºé—´: {action_dim}")
    print(f"è®­ç»ƒç§å­: {TRAINING_SEED}")
    print(f"æœ€å¤§è®­ç»ƒè½®æ•°: {MAX_EPISODES}")
    print(f"è¯„ä¼°é—´éš”: æ¯{EVAL_INTERVAL}è½®")
    print()

    # é¢„çƒ­ç»éªŒæ± 
    print(f"é¢„çƒ­ç»éªŒæ±  (ç›®æ ‡: {MEMORY_WARMUP_SIZE})...")
    warmup_count = 0
    while len(memory) < MEMORY_WARMUP_SIZE:
        run_training_episode(env, agent, memory)
        warmup_count += 1
        if warmup_count % 100 == 0:
            print(f"  é¢„çƒ­è¿›åº¦: {len(memory)}/{MEMORY_WARMUP_SIZE}")
    print(f"âœ… ç»éªŒæ± é¢„çƒ­å®Œæˆï¼Œå…± {len(memory)} æ¡ç»éªŒ\n")

    # è®­ç»ƒæ•°æ®è®°å½•
    training_log = {
        'config': {
            'seed': TRAINING_SEED,
            'max_episodes': MAX_EPISODES,
            'memory_size': MEMORY_SIZE,
            'batch_size': BATCH_SIZE,
            'learning_rate': LEARNING_RATE,
            'gamma': GAMMA,
            'eval_interval': EVAL_INTERVAL,
        },
        'training': [],
        'validation': [],
        'test': []
    }

    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    print("-" * 70)

    best_val_score = 0

    for episode in range(MAX_EPISODES + 1):
        # è®­ç»ƒä¸€ä¸ªepisode
        episode_metrics = run_training_episode(env, agent, memory)

        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        training_log['training'].append({
            'episode': episode,
            **episode_metrics
        })

        # å®šæœŸè¯„ä¼°
        if episode % EVAL_INTERVAL == 0:
            # éªŒè¯é›†è¯„ä¼°
            val_metrics = evaluate_agent(env, agent, VAL_SEEDS, EVAL_EPISODES)
            training_log['validation'].append({
                'episode': episode,
                **val_metrics
            })

            print(f"\nEpisode {episode}/{MAX_EPISODES}")
            print(f"  è®­ç»ƒ - åˆ†æ•°: {episode_metrics['score']:.0f}, "
                  f"å¥–åŠ±: {episode_metrics['reward']:.1f}, "
                  f"Loss: {episode_metrics['mean_loss']:.4f}")
            print(f"  éªŒè¯ - åˆ†æ•°: {val_metrics['mean_score']:.1f}Â±{val_metrics['std_score']:.1f}, "
                  f"å¥–åŠ±: {val_metrics['mean_reward']:.1f}Â±{val_metrics['std_reward']:.1f}")
            print(f"  Îµ-greedy: {agent.e_greed:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['mean_score'] > best_val_score:
                best_val_score = val_metrics['mean_score']
                best_model_path = os.path.join(MODEL_SAVE_DIR, "best_model.pdparams")
                paddle.save(agent.policy_net.state_dict(), best_model_path)
                print(f"  ğŸ† æ–°æœ€ä½³éªŒè¯åˆ†æ•°! æ¨¡å‹å·²ä¿å­˜")

            print("-" * 70)

        # æ˜¾ç¤ºè¿›åº¦
        elif episode % 10 == 0:
            progress = episode / MAX_EPISODES * 100
            print(f"[{progress:5.1f}%] Episode {episode:4d}, "
                  f"Score: {episode_metrics['score']:3.0f}, "
                  f"Reward: {episode_metrics['reward']:6.1f}, "
                  f"Loss: {episode_metrics['mean_loss']:.4f}, "
                  f"Îµ: {agent.e_greed:.4f}",
                  end='\r')

    print("\n")

    # æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°
    print("æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°...")
    test_metrics = evaluate_agent(env, agent, TEST_SEEDS, len(TEST_SEEDS))
    training_log['test'].append({
        'episode': MAX_EPISODES,
        **test_metrics
    })

    print(f"æµ‹è¯•é›†ç»“æœ - åˆ†æ•°: {test_metrics['mean_score']:.1f}Â±{test_metrics['std_score']:.1f}, "
          f"å¥–åŠ±: {test_metrics['mean_reward']:.1f}Â±{test_metrics['std_reward']:.1f}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "final.pdparams"
    paddle.save(agent.policy_net.state_dict(), final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    with open(LOG_FILE, 'w') as f:
        json.dump(training_log, f, indent=2)
    print(f"âœ… è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {LOG_FILE}")

    print("\n" + "=" * 70)
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯åˆ†æ•°: {best_val_score:.1f}")
    print(f"æœ€ç»ˆæµ‹è¯•åˆ†æ•°: {test_metrics['mean_score']:.1f}")
    print("=" * 70)


if __name__ == "__main__":
    main()
