"""
å¯¹æ¯”MCTSå’ŒDQNçš„è¡¨ç°
"""
import sys
import os
import numpy as np
import time
import paddle

from GameInterface import GameInterface
from DQN import Agent, build_model, RandomAgent

# MCTSæš‚æ—¶ç”¨éšæœºä»£æ›¿ï¼Œå¦‚æœèƒ½åŠ è½½å°±ç”¨MCTS
USE_MCTS = False
try:
    from mcts.MCTS_optimized import FastMCTSAgent
    USE_MCTS = True
    print("âœ“ MCTSæ¨¡å—åŠ è½½æˆåŠŸ")
except:
    print("âœ— MCTSæ¨¡å—åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨éšæœºAgentå¯¹æ¯”")


def evaluate_agent(agent, env, num_games=10, agent_name="Agent"):
    """è¯„ä¼°ä¸€ä¸ªagent"""
    scores = []
    steps_list = []
    times_per_move = []

    for game_idx in range(num_games):
        env.reset(seed=game_idx * 100)

        action = np.random.randint(0, env.action_num)
        feature, _, alive = env.next(action)

        step = 0
        total_time = 0

        while alive:
            step += 1

            start = time.time()
            if hasattr(agent, 'predict'):
                if agent_name == "MCTS":
                    action = agent.predict(env)
                else:
                    action = agent.predict(feature)
            else:
                action = agent.sample(feature)
            elapsed = time.time() - start
            total_time += elapsed

            # å¤„ç†actionï¼šå¯èƒ½æ˜¯numpyæ ‡é‡ã€numpyæ•°ç»„æˆ–æ•´æ•°
            if isinstance(action, np.ndarray):
                if action.ndim == 0:  # 0ç»´æ•°ç»„ï¼ˆæ ‡é‡ï¼‰
                    action_val = int(action)
                else:
                    action_val = int(action[0])
            else:
                action_val = int(action)

            feature, reward, alive = env.next(action_val)

        final_score = env.game.score
        avg_time = total_time / step if step > 0 else 0

        scores.append(final_score)
        steps_list.append(step)
        times_per_move.append(avg_time)

        print(f"  æ¸¸æˆ {game_idx + 1}/{num_games}: å¾—åˆ†={final_score:4d}, æ­¥æ•°={step:3d}, æ—¶é—´={avg_time:.4f}s/æ­¥")

    return {
        'agent': agent_name,
        'scores': scores,
        'steps': steps_list,
        'times': times_per_move,
        'mean_score': np.mean(scores),
        'std_score': np.std(scores),
        'max_score': np.max(scores),
        'min_score': np.min(scores),
        'mean_time': np.mean(times_per_move)
    }


def main():
    print("=" * 70)
    print("ğŸ® DQN vs MCTS å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)

    # åˆå§‹åŒ–ç¯å¢ƒ
    env = GameInterface()
    feature_dim = GameInterface.FEATURE_MAP_WIDTH * GameInterface.FEATURE_MAP_HEIGHT * 2
    action_dim = env.action_num

    print(f"ç¯å¢ƒé…ç½®: feature_dim={feature_dim}, action_dim={action_dim}")

    num_games = 5  # æ¯ä¸ªagentæµ‹è¯•5å±€

    results = {}

    # 1. æµ‹è¯•éšæœºAgentï¼ˆåŸºå‡†ï¼‰
    print("\n[1/3] æµ‹è¯•éšæœºAgentï¼ˆåŸºå‡†ï¼‰")
    print("-" * 70)
    random_agent = RandomAgent(action_dim)
    results['Random'] = evaluate_agent(random_agent, env, num_games, "Random")

    # 2. æµ‹è¯•DQN Agent
    print("\n[2/3] æµ‹è¯•DQN Agent")
    print("-" * 70)
    dqn_agent = Agent(build_model, feature_dim, action_dim, e_greed=0.0)

    # å°è¯•åŠ è½½æ¨¡å‹
    model_paths = ['final_5000.pdparams', 'final.pdparams']
    loaded = False
    for model_path in model_paths:
        if os.path.exists(model_path):
            print(f"  åŠ è½½æ¨¡å‹: {model_path}")
            dqn_agent.policy_net.set_state_dict(paddle.load(model_path))
            loaded = True
            break

    if not loaded:
        print("  âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒçš„DQN")

    results['DQN'] = evaluate_agent(dqn_agent, env, num_games, "DQN")

    # 3. æµ‹è¯•MCTS Agent
    print("\n[3/3] æµ‹è¯•MCTS Agent")
    print("-" * 70)
    if USE_MCTS:
        mcts_agent = FastMCTSAgent(num_simulations=100)
        results['MCTS'] = evaluate_agent(mcts_agent, env, num_games, "MCTS")
    else:
        print("  è·³è¿‡MCTSæµ‹è¯•ï¼ˆæ¨¡å—æœªåŠ è½½ï¼‰")

    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”ç»“æœ")
    print("=" * 70)

    print(f"\n{'Agent':<12} {'å¹³å‡å¾—åˆ†':<15} {'æœ€é«˜åˆ†':<10} {'æœ€ä½åˆ†':<10} {'å¹³å‡æ—¶é—´(s/æ­¥)':<15}")
    print("-" * 70)

    for agent_name in ['Random', 'DQN', 'MCTS']:
        if agent_name not in results:
            continue
        r = results[agent_name]
        print(f"{r['agent']:<12} {r['mean_score']:>6.1f} Â± {r['std_score']:<5.1f} "
              f"{r['max_score']:>10} {r['min_score']:>10} {r['mean_time']:>15.4f}")

    # å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 70)
    print("ğŸ¯ å¯¹æ¯”åˆ†æ")
    print("=" * 70)

    baseline = results['Random']['mean_score']

    for agent_name in ['DQN', 'MCTS']:
        if agent_name not in results:
            continue
        r = results[agent_name]
        improvement = ((r['mean_score'] - baseline) / baseline * 100) if baseline > 0 else 0

        print(f"\n{agent_name} vs Random:")
        print(f"  å¹³å‡å¾—åˆ†æå‡: {r['mean_score'] - baseline:+.1f} ({improvement:+.1f}%)")
        print(f"  æœ€é«˜åˆ†æå‡: {r['max_score'] - results['Random']['max_score']:+d}")
        print(f"  è®¡ç®—é€Ÿåº¦: {r['mean_time']:.4f}ç§’/æ­¥")

        if r['mean_score'] > baseline * 1.5:
            print(f"  âœ… {agent_name}æ˜¾è‘—ä¼˜äºéšæœºç­–ç•¥")
        elif r['mean_score'] > baseline:
            print(f"  ğŸ”¸ {agent_name}ç•¥ä¼˜äºéšæœºç­–ç•¥")
        else:
            print(f"  âŒ {agent_name}æœªè¶…è¶Šéšæœºç­–ç•¥")

    # DQN vs MCTS
    if 'DQN' in results and 'MCTS' in results:
        print(f"\nDQN vs MCTS:")
        dqn_score = results['DQN']['mean_score']
        mcts_score = results['MCTS']['mean_score']
        diff = dqn_score - mcts_score

        if abs(diff) < 5:
            print(f"  ğŸ¤ æ€§èƒ½æ¥è¿‘ (å·®è·: {abs(diff):.1f}åˆ†)")
        elif dqn_score > mcts_score:
            print(f"  ğŸ† DQNèƒœå‡º (+{diff:.1f}åˆ†, {diff/mcts_score*100:+.1f}%)")
        else:
            print(f"  ğŸ† MCTSèƒœå‡º (+{abs(diff):.1f}åˆ†, {abs(diff)/dqn_score*100:+.1f}%)")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    main()
