# MCTS 决策原理详解

## 🎯 从演示中看到的决策过程

### 典型决策示例（第3步）

```
📊 搜索统计:
  总模拟次数: 200
  扩展节点数: 10
  思考时间: 0.120秒
  速度: 1673 rollouts/秒

🏆 候选动作排名:
  排名  列  访问次数  平均价值  选择概率
  👉1   6   183      32.7      91.5%  ← 最佳选择
    2   4   3        21.3      1.5%
    3   3   3        22.0      1.5%
    4   2   3        22.0      1.5%
    5   8   3        23.7      1.5%
```

**关键观察：**
- ✅ 第6列被访问了183次（91.5%）→ **最可靠的选择**
- ✅ 平均价值32.7 → **预期收益最高**
- ⚠️ 其他列只被访问3次 → 探索后发现不太好

---

## 🔍 MCTS如何决策（4个阶段）

### 阶段1: Selection (选择)

从根节点开始，使用PUCT公式选择最有希望的路径：

```
PUCT = Q(s,a) + 1.5 * P(a|s) * sqrt(N(s)) / (1 + N(s,a))
       ↑              ↑
    利用分数      探索分数
```

**示例：**
```
动作6: Q=32.7, visits=183 → PUCT = 32.7 + 0.2 = 32.9
动作4: Q=21.3, visits=3   → PUCT = 21.3 + 3.5 = 24.8
```

**结论：** 动作6的PUCT更高，但如果某个动作访问次数太少，探索分数会补偿它。

### 阶段2: Expansion (扩展)

到达叶节点后，添加新的子节点。

**Progressive Widening（防爆炸关键）：**
```python
max_children = 3 + sqrt(visit_count)

visit_count=0  → 最多3个子节点
visit_count=4  → 最多5个子节点
visit_count=16 → 最多7个子节点
visit_count=64 → 最多11个子节点
```

**效果：**
- 初始只探索3-5个最有希望的动作
- 随着访问增加，逐步扩展更多选项
- **防止树爆炸** ✅

### 阶段3: Simulation (模拟)

从新节点快速模拟到游戏结束，使用**启发式策略**：

```python
def score_action(state, action):
    score = 0

    # 1. 中心偏好 (+2分)
    center_dist = abs(col - 5)
    score += 2.0 * (1 - center_dist/5)

    # 2. 高度惩罚 (-2分)
    column_height = get_height(col)
    score -= 2.0 * (height / max_height)

    # 3. 合并奖励 (如果实现)
    merge_potential = count_adjacent_same(col)
    score += 5.0 * merge_potential

    return score
```

**为什么不用随机？**
- ❌ 随机模拟：太慢，信号弱
- ✅ 启发式模拟：快10倍，质量更好

### 阶段4: Backpropagation (反向传播)

更新路径上所有节点的统计：

```python
path = [root, child1, child2, ..., leaf]
value = simulation_result  # 例如：32.7

for node in path:
    node.visit_count += 1
    node.total_value += value
```

**效果：**
- 好的路径：visit_count↑, total_value↑
- 差的路径：被访问几次后就放弃

---

## 📈 为什么访问次数 = 可靠性？

### 数学原理（大数定律）

```
Q(s,a) = total_value / visit_count

visit_count = 1:   Q = 32.0  (单次结果，不可靠)
visit_count = 10:  Q = 31.5  (稍微稳定)
visit_count = 100: Q = 32.7  (非常可靠!) ✅
```

### 实际例子

**动作6（被访问183次）：**
```
模拟1:  得分28 → Q = 28.0
模拟5:  平均30 → Q = 30.2
模拟50: 平均32 → Q = 32.1
模拟183:平均33 → Q = 32.7 ← 稳定！
```

**动作4（只访问3次）：**
```
模拟1: 得分20 → Q = 20.0
模拟2: 得分25 → Q = 22.5
模拟3: 得分18 → Q = 21.3 ← 不稳定，可能误导
```

**结论：** 访问次数多 = 估值更准确 = 更可靠

---

## 🎲 探索 vs 利用的平衡

### PUCT公式的作用

```
PUCT = Q(s,a)        +  c_puct * P(a) * sqrt(N(s)) / (1+N(s,a))
       ↑                ↑
    利用（选好的）      探索（试新的）
```

### 动态平衡示例

**初期（N(s) = 10）：**
```
动作A: Q=10, visits=8  → PUCT = 10 + 1.5*0.5*3.2/9  = 10.3
动作B: Q=8,  visits=2  → PUCT = 8  + 1.5*0.5*3.2/3  = 8.8
```
→ 选择A（利用已知好的）

**后期（N(s) = 100）：**
```
动作A: Q=10, visits=80 → PUCT = 10 + 1.5*0.5*10/81 = 10.1
动作B: Q=8,  visits=2  → PUCT = 8  + 1.5*0.5*10/3  = 10.5
```
→ 选择B（探索未充分尝试的）✨

**效果：**
- 早期：重点利用好的动作
- 后期：确保没遗漏更好的选择

---

## 🧠 MCTS vs 其他方法

### vs 深度优先搜索（DFS）

| 特性 | DFS | MCTS |
|------|-----|------|
| 搜索方式 | 穷尽所有路径 | 聚焦有希望的路径 |
| 深度 | 固定深度 | 自适应深度 |
| 质量 | 完美（如果搜完） | 近似最优 |
| 速度 | 指数爆炸 💥 | 线性增长 ✅ |

### vs 贪心算法

| 特性 | 贪心 | MCTS |
|------|------|------|
| 前瞻 | 只看当前步 | 看未来30-50步 |
| 质量 | 容易陷入局部最优 | 全局视角 |
| 速度 | 极快 (0.001秒) | 快 (0.1秒) |

### vs DQN神经网络

| 特性 | DQN | MCTS |
|------|-----|------|
| 训练 | 需要2000+局 | 无需训练 ✅ |
| 推理 | 快 (0.01秒) | 慢 (0.1秒) |
| 可解释 | 黑盒 ❌ | 透明 ✅ |
| 适应性 | 需重训练 | 即时适应 ✅ |

---

## 🎯 实战中的决策案例

### 案例1: 选择合并机会

**状态：**
```
底部：①②①
当前水果：①
```

**MCTS分析：**
```
动作A: 放在左边的① → 模拟200次，平均得分+2
动作B: 放在右边的① → 模拟200次，平均得分+2
动作C: 放在中间空位  → 模拟200次，平均得分-1
```

**结果：** 选择A或B（自动发现合并机会）✅

### 案例2: 避免堆积

**状态：**
```
左边已经很高（15层）
右边还是空的
```

**MCTS分析：**
```
动作A: 放在左边 → 模拟100次，80次导致游戏结束
动作B: 放在右边 → 模拟100次，0次游戏结束
```

**结果：** 选择B（自动避开危险区域）✅

### 案例3: 长期规划

**状态：**
```
可以立即得2分，但会堵住未来合并路径
可以得0分，但为未来大合并创造条件
```

**MCTS分析：**
```
贪心: 选择立即+2分
MCTS: 模拟30步后，发现"得0分"的选择未来可得+20分
```

**结果：** MCTS选择长期收益 ✅

---

## 💡 为什么MCTS有效？

### 1. 智能采样
不是随机搜索，而是**聚焦在有希望的区域**：
- 好的分支：访问100+次
- 差的分支：访问2-3次就放弃

### 2. 统计保证
通过大量模拟，**收敛到真实价值**：
```
模拟次数越多 → 估值越准 → 决策越好
```

### 3. 自适应
自动调整：
- 简单局面：快速收敛
- 复杂局面：持续探索

### 4. 可扩展
- 200次模拟：良好
- 500次模拟：优秀
- 2000次模拟：顶级
- 10000次模拟：近乎完美

---

## 🎓 实践建议

### 调参指南

**追求速度：**
```python
num_simulations = 50-100
MAX_SIMULATION_DEPTH = 20
```

**平衡质量：**
```python
num_simulations = 200-300  ← 推荐
MAX_SIMULATION_DEPTH = 30
```

**追求最优：**
```python
num_simulations = 500-1000
MAX_SIMULATION_DEPTH = 50
```

### 观察指标

**好的搜索：**
- ✅ 最佳动作访问率 > 80%
- ✅ 扩展节点数 = 5-15个
- ✅ 平均价值稳定增长

**需要调整：**
- ⚠️ 最佳动作访问率 < 50% → 增加模拟次数
- ⚠️ 扩展节点数 < 3 → 太保守，调大progressive widening
- ⚠️ 扩展节点数 > 20 → 太激进，可能爆炸

---

## 📊 性能分析

### 时间分配

**单次搜索（200次模拟，0.15秒）：**
```
Selection:    10% (0.015秒)
Expansion:     5% (0.007秒)
Simulation:   80% (0.120秒) ← 瓶颈
Backprop:      5% (0.007秒)
```

**优化重点：** 加速Simulation！

### 质量 vs 速度权衡

```
50次模拟:   0.04秒，得分~150
100次模拟:  0.08秒，得分~200
200次模拟:  0.15秒，得分~250  ← 最佳平衡
500次模拟:  0.45秒，得分~300
1000次模拟: 1.00秒，得分~320
```

**边际收益递减：** 200次是甜点 ✅

---

## ✅ 总结

### MCTS决策的核心逻辑

1. **通过模拟探索未来**
   - 不是猜测，是实际"试玩"200次

2. **访问次数投票**
   - 访问次数 = 信任度
   - 183次 vs 3次 → 前者胜出

3. **自动平衡探索和利用**
   - PUCT公式自动调节
   - 既找好的，也试新的

4. **统计收敛保证**
   - 模拟越多，越接近真实价值
   - 大数定律保证正确性

### 为什么比DQN更可解释？

**DQN：**
```
输入 → 神经网络 → 输出动作6
              ↑
            黑盒
```

**MCTS：**
```
输入 → 200次模拟 → 统计结果 → 动作6
       183次选它    Q=32.7
        3次选它     Q=21.3
        ↑
      完全透明！
```

你可以看到**每一次模拟**，理解**为什么选择这个动作**！
