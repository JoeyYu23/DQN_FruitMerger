#!/usr/bin/env python3
"""
å¯è§†åŒ–AlphaZeroè®­ç»ƒç»“æœ
"""

import json
import os
import matplotlib.pyplot as plt
import numpy as np
from typing import Optional


def visualize_training_history(checkpoint_dir: str = "weights/alphazero",
                               save_path: Optional[str] = None,
                               show: bool = True):
    """
    å¯è§†åŒ–è®­ç»ƒå†å²

    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
        save_path: ä¿å­˜å›¾ç‰‡è·¯å¾„ (Noneåˆ™ä¸ä¿å­˜)
        show: æ˜¯å¦æ˜¾ç¤ºå›¾ç‰‡
    """
    history_path = os.path.join(checkpoint_dir, "history.json")

    if not os.path.exists(history_path):
        print(f"âŒ æ‰¾ä¸åˆ°å†å²æ–‡ä»¶: {history_path}")
        print(f"   è¯·å…ˆè¿è¡Œè®­ç»ƒ: python run_training.py train")
        return

    # è¯»å–å†å²æ•°æ®
    with open(history_path, 'r') as f:
        history = json.load(f)

    # æ£€æŸ¥æ•°æ®
    if not history.get('iterations'):
        print("âŒ å†å²æ•°æ®ä¸ºç©º")
        return

    iterations = history['iterations']
    train_losses = history.get('train_losses', [])
    policy_losses = history.get('policy_losses', [])
    value_losses = history.get('value_losses', [])
    eval_scores = history.get('eval_scores', [])

    print(f"\nğŸ“Š è®­ç»ƒå†å²ç»Ÿè®¡:")
    print(f"   æ€»è¿­ä»£æ¬¡æ•°: {len(iterations)}")
    print(f"   æœ€ç»ˆè®­ç»ƒLoss: {train_losses[-1]:.4f}" if train_losses else "   æ— è®­ç»ƒLossæ•°æ®")
    print(f"   æœ€ç»ˆè¯„ä¼°åˆ†æ•°: {eval_scores[-1]:.1f}" if eval_scores else "   æ— è¯„ä¼°åˆ†æ•°æ•°æ®")

    if eval_scores:
        print(f"   æœ€é«˜è¯„ä¼°åˆ†æ•°: {max(eval_scores):.1f}")
        print(f"   å¹³å‡è¯„ä¼°åˆ†æ•°: {np.mean(eval_scores):.1f}")

    # åˆ›å»ºå›¾è¡¨
    fig = plt.figure(figsize=(15, 10))

    # 2è¡Œ2åˆ—å¸ƒå±€
    # 1. è®­ç»ƒLoss
    ax1 = plt.subplot(2, 2, 1)
    if train_losses:
        ax1.plot(iterations, train_losses, 'b-', linewidth=2, label='Total Loss')
        ax1.set_xlabel('Iteration', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.set_title('Training Loss', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend()

    # 2. Policyå’ŒValue Losså¯¹æ¯”
    ax2 = plt.subplot(2, 2, 2)
    if policy_losses and value_losses:
        ax2.plot(iterations, policy_losses, 'r-', linewidth=2, label='Policy Loss')
        ax2.plot(iterations, value_losses, 'g-', linewidth=2, label='Value Loss')
        ax2.set_xlabel('Iteration', fontsize=12)
        ax2.set_ylabel('Loss', fontsize=12)
        ax2.set_title('Policy vs Value Loss', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.legend()

    # 3. è¯„ä¼°åˆ†æ•°
    ax3 = plt.subplot(2, 2, 3)
    if eval_scores:
        ax3.plot(iterations, eval_scores, 'purple', linewidth=2, marker='o',
                markersize=6, label='Eval Score')
        ax3.set_xlabel('Iteration', fontsize=12)
        ax3.set_ylabel('Score', fontsize=12)
        ax3.set_title('Evaluation Score Progress', fontsize=14, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        ax3.legend()

        # æ·»åŠ æœ€é«˜åˆ†æ ‡è®°
        max_idx = np.argmax(eval_scores)
        ax3.annotate(f'Max: {eval_scores[max_idx]:.1f}',
                    xy=(iterations[max_idx], eval_scores[max_idx]),
                    xytext=(10, 10), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),
                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))

    # 4. åˆ†æ•°å˜åŒ–ç‡
    ax4 = plt.subplot(2, 2, 4)
    if eval_scores and len(eval_scores) > 1:
        score_diff = np.diff(eval_scores)
        ax4.bar(iterations[1:], score_diff, color=['green' if x > 0 else 'red' for x in score_diff],
               alpha=0.7)
        ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax4.set_xlabel('Iteration', fontsize=12)
        ax4.set_ylabel('Score Change', fontsize=12)
        ax4.set_title('Score Improvement per Iteration', fontsize=14, fontweight='bold')
        ax4.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    # ä¿å­˜
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")

    # æ˜¾ç¤º
    if show:
        plt.show()

    return fig


def print_training_summary(checkpoint_dir: str = "weights/alphazero"):
    """æ‰“å°è®­ç»ƒæ‘˜è¦"""
    history_path = os.path.join(checkpoint_dir, "history.json")

    if not os.path.exists(history_path):
        print(f"âŒ æ‰¾ä¸åˆ°å†å²æ–‡ä»¶: {history_path}")
        return

    with open(history_path, 'r') as f:
        history = json.load(f)

    iterations = history.get('iterations', [])
    eval_scores = history.get('eval_scores', [])

    if not iterations:
        print("âŒ æ— è®­ç»ƒæ•°æ®")
        return

    print("\n" + "="*60)
    print("  è®­ç»ƒæ‘˜è¦")
    print("="*60)
    print(f"è¿­ä»£æ¬¡æ•°: {len(iterations)}")

    if eval_scores:
        print(f"\nè¯„ä¼°åˆ†æ•°:")
        print(f"  åˆå§‹: {eval_scores[0]:.1f}")
        print(f"  æœ€ç»ˆ: {eval_scores[-1]:.1f}")
        print(f"  æœ€é«˜: {max(eval_scores):.1f}")
        print(f"  å¹³å‡: {np.mean(eval_scores):.1f}")
        print(f"  æå‡: {eval_scores[-1] - eval_scores[0]:.1f} ({((eval_scores[-1]/eval_scores[0]-1)*100):.1f}%)")

    print("="*60)


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='å¯è§†åŒ–AlphaZeroè®­ç»ƒç»“æœ')
    parser.add_argument('--checkpoint-dir', type=str, default='weights/alphazero',
                       help='æ£€æŸ¥ç‚¹ç›®å½•')
    parser.add_argument('--save-path', type=str, default='training_visualization.png',
                       help='ä¿å­˜å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--no-show', action='store_true',
                       help='ä¸æ˜¾ç¤ºå›¾ç‰‡')
    parser.add_argument('--summary-only', action='store_true',
                       help='åªæ‰“å°æ‘˜è¦')

    args = parser.parse_args()

    if args.summary_only:
        print_training_summary(args.checkpoint_dir)
    else:
        visualize_training_history(
            checkpoint_dir=args.checkpoint_dir,
            save_path=args.save_path,
            show=not args.no_show
        )
        print_training_summary(args.checkpoint_dir)
