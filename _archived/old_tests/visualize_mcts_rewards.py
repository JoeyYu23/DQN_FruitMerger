#!/usr/bin/env python3
"""
å¯è§†åŒ–æ¯ä¸€æ­¥MCTSæ‰€æœ‰ä½ç½®çš„reward/Qå€¼
"""

from GameInterface import GameInterface
from mcts.MCTS_real_physics import RealPhysicsMCTSAgent, RealPhysicsNode
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

def visualize_step_rewards(
    env,
    agent,
    step,
    output_dir
):
    """
    å¯è§†åŒ–å½“å‰æ­¥éª¤æ‰€æœ‰ä½ç½®çš„reward

    Args:
        env: æ¸¸æˆç¯å¢ƒ
        agent: MCTSæ™ºèƒ½ä½“
        step: å½“å‰æ­¥æ•°
        output_dir: è¾“å‡ºç›®å½•
    """
    # ä¿å­˜çŠ¶æ€
    original_state = agent.mcts._save_state(env)

    # ç›´æ¥è®¡ç®—æ¯ä¸ªactionçš„ä¸¤æ­¥å‰ç»reward
    action_rewards = {}
    action_segment_len = env.game.width / 16

    for action1 in range(16):
        # æ¢å¤åˆ°åˆå§‹çŠ¶æ€
        agent.mcts._restore_state(env, original_state)

        # è®°å½•åˆå§‹çŠ¶æ€
        score_before = env.game.score
        fruits_before = agent.mcts._get_fruits_info(env)

        # æ‰§è¡Œç¬¬ä¸€æ­¥
        agent.mcts._apply_action(env, action1)

        # è®¡ç®—ç¬¬ä¸€æ­¥reward
        reward1 = agent.mcts._calculate_reward(env, score_before, fruits_before)

        # è®°å½•ç¬¬ä¸€æ­¥åçš„çŠ¶æ€
        state_after_step1 = agent.mcts._save_state(env)
        score_after1 = env.game.score
        fruits_after1 = agent.mcts._get_fruits_info(env)

        # è®¡ç®—æ‰€æœ‰ç¬¬äºŒæ­¥çš„rewardï¼Œæ‰¾æœ€å¤§å€¼
        max_reward2 = float('-inf')

        for action2 in range(16):
            # æ¢å¤åˆ°ç¬¬ä¸€æ­¥åçš„çŠ¶æ€
            agent.mcts._restore_state(env, state_after_step1)

            # æ‰§è¡Œç¬¬äºŒæ­¥
            agent.mcts._apply_action(env, action2)

            # è®¡ç®—ç¬¬äºŒæ­¥reward
            reward2 = agent.mcts._calculate_reward(env, score_after1, fruits_after1)

            # æ›´æ–°æœ€å¤§å€¼
            if reward2 > max_reward2:
                max_reward2 = reward2

        # æ€»reward = ç¬¬ä¸€æ­¥reward + ç¬¬äºŒæ­¥æœ€å¤§reward
        action_rewards[action1] = reward1 + max_reward2

    # æ¢å¤çŠ¶æ€
    agent.mcts._restore_state(env, original_state)

    # æ”¶é›†æ¯ä¸ªåŠ¨ä½œçš„ç»Ÿè®¡ä¿¡æ¯
    action_stats = []
    for action in range(16):
        q_value = action_rewards.get(action, 0.0)

        action_stats.append({
            'action': action,
            'q_value': q_value,
            'visits': 1,  # æ¯ä¸ªéƒ½è®¡ç®—äº†1æ¬¡
            'puct': q_value  # PUCTå°±ç”¨Qå€¼
        })

    # è·å–æ¸¸æˆç”»é¢ï¼ˆå…ˆæ¸²æŸ“ï¼‰
    env.game.draw()  # æ¸²æŸ“å½“å‰çŠ¶æ€
    game_frame = env.game.screen.copy()

    # åˆ›å»ºå¯è§†åŒ–
    fig = plt.figure(figsize=(16, 10))

    # 1. æ¸¸æˆç”»é¢ + ä½ç½®æ ‡è®°
    ax1 = plt.subplot(2, 3, (1, 4))
    game_rgb = cv2.cvtColor(game_frame, cv2.COLOR_RGBA2RGB)
    ax1.imshow(game_rgb)
    ax1.set_title(f'Step {step} - Game State', fontsize=14, fontweight='bold')
    ax1.axis('off')

    # åœ¨æ¸¸æˆç”»é¢ä¸Šæ ‡è®°æ¯ä¸ªä½ç½®çš„Qå€¼
    game_width = env.game.width
    action_segment_len = game_width / 16

    # æ‰¾å‡ºQå€¼èŒƒå›´ç”¨äºå½’ä¸€åŒ–é¢œè‰²
    q_values = [s['q_value'] for s in action_stats]
    if max(q_values) > min(q_values):
        q_min, q_max = min(q_values), max(q_values)
    else:
        q_min, q_max = 0, 1

    # åœ¨æ¯ä¸ªä½ç½®ç”»åœ†åœˆå’Œæ•°å€¼
    game_rgb_marked = game_rgb.copy()  # åˆ›å»ºä¸€æ¬¡å‰¯æœ¬

    for stat in action_stats:
        action = stat['action']
        q_value = stat['q_value']
        visits = stat['visits']

        x = int((action + 0.5) * action_segment_len)
        y = 30  # é¡¶éƒ¨ä½ç½®

        # é¢œè‰²æ˜ å°„ï¼šQå€¼è¶Šé«˜è¶Šç»¿ï¼Œè¶Šä½è¶Šçº¢
        if q_max > q_min:
            normalized_q = (q_value - q_min) / (q_max - q_min)
        else:
            normalized_q = 0.5

        color_r = int(255 * (1 - normalized_q))
        color_g = int(255 * normalized_q)

        # ç”»åœ†åœˆå’Œè®¿é—®æ¬¡æ•°
        cv2.circle(game_rgb_marked, (x, y), 12, (color_r, color_g, 0), -1)
        cv2.circle(game_rgb_marked, (x, y), 12, (255, 255, 255), 1)  # ç™½è‰²è¾¹æ¡†

        # ç”»è®¿é—®æ¬¡æ•°
        cv2.putText(game_rgb_marked, f'{visits}', (x-6, y+4),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)

    ax1.imshow(game_rgb_marked)

    # 2. Qå€¼æŸ±çŠ¶å›¾
    ax2 = plt.subplot(2, 3, 2)
    actions = [s['action'] for s in action_stats]
    q_values = [s['q_value'] for s in action_stats]
    colors = plt.cm.RdYlGn([(q - q_min) / (q_max - q_min + 1e-6) for q in q_values])

    bars = ax2.bar(actions, q_values, color=colors, alpha=0.8, edgecolor='black')
    ax2.set_xlabel('Action (Position)', fontsize=11)
    ax2.set_ylabel('Q-Value', fontsize=11)
    ax2.set_title('Q-Value per Action', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.set_xticks(range(0, 16, 2))

    # æ ‡è®°æœ€ä½³åŠ¨ä½œï¼ˆé€‰Qå€¼æœ€å¤§çš„ï¼‰
    best_action = max(action_stats, key=lambda x: x['q_value'])['action']
    ax2.axvline(best_action, color='red', linestyle='--', linewidth=2,
                label=f'Best: {best_action}')
    ax2.legend()

    # 3. è®¿é—®æ¬¡æ•°æŸ±çŠ¶å›¾
    ax3 = plt.subplot(2, 3, 3)
    visits = [s['visits'] for s in action_stats]
    ax3.bar(actions, visits, color='skyblue', alpha=0.8, edgecolor='black')
    ax3.set_xlabel('Action (Position)', fontsize=11)
    ax3.set_ylabel('Visit Count', fontsize=11)
    ax3.set_title('Visit Count per Action', fontsize=12, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    ax3.set_xticks(range(0, 16, 2))
    ax3.axvline(best_action, color='red', linestyle='--', linewidth=2)

    # 4. PUCTå€¼æŸ±çŠ¶å›¾
    ax4 = plt.subplot(2, 3, 5)
    puct_values = [s['puct'] for s in action_stats]
    ax4.bar(actions, puct_values, color='orange', alpha=0.8, edgecolor='black')
    ax4.set_xlabel('Action (Position)', fontsize=11)
    ax4.set_ylabel('PUCT Value', fontsize=11)
    ax4.set_title('PUCT Value per Action', fontsize=12, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.set_xticks(range(0, 16, 2))
    ax4.axvline(best_action, color='red', linestyle='--', linewidth=2)

    # 5. ç»Ÿè®¡è¡¨æ ¼
    ax5 = plt.subplot(2, 3, 6)
    ax5.axis('off')

    # é€‰æ‹©top 5åŠ¨ä½œ
    top_actions = sorted(action_stats, key=lambda x: x['visits'], reverse=True)[:5]

    table_data = []
    for stat in top_actions:
        table_data.append([
            f"{stat['action']}",
            f"{stat['q_value']:.2f}",
            f"{stat['visits']}",
            f"{stat['puct']:.2f}"
        ])

    table = ax5.table(
        cellText=table_data,
        colLabels=['Action', 'Q-Value', 'Visits', 'PUCT'],
        cellLoc='center',
        loc='center',
        bbox=[0, 0.3, 1, 0.6]
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    # æ ‡é¢˜
    ax5.text(0.5, 0.95, 'Top 5 Actions',
            ha='center', va='top', fontsize=12, fontweight='bold')

    # æ¸¸æˆä¿¡æ¯
    info_text = f"Score: {env.game.score}\n"
    info_text += f"Fruits: {len(env.game.fruits)}\n"
    info_text += f"Simulations: {agent.num_simulations}"
    ax5.text(0.5, 0.15, info_text,
            ha='center', va='top', fontsize=10,
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()

    # ä¿å­˜
    output_path = os.path.join(output_dir, f'step_{step:03d}_rewards.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    # è¿”å›æœ€ä½³åŠ¨ä½œ
    return best_action


def run_game_with_visualization(
    seed=888,
    num_sims=50,
    max_steps=50,
    output_dir='mcts_rewards_visualization'
):
    """è¿è¡Œæ¸¸æˆå¹¶å¯è§†åŒ–æ¯ä¸€æ­¥çš„reward"""

    print("="*70)
    print("ğŸ¨ MCTS Rewards å¯è§†åŒ–")
    print("="*70)
    print(f"é…ç½®:")
    print(f"  Seed: {seed}")
    print(f"  Simulations: {num_sims}")
    print(f"  Max Steps: {max_steps}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print("="*70)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"\nâœ… åˆ›å»ºç›®å½•: {output_dir}")

    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = GameInterface()
    agent = RealPhysicsMCTSAgent(num_simulations=num_sims)

    # é‡ç½®æ¸¸æˆ
    env.reset(seed=seed)

    # ç¬¬ä¸€æ­¥éšæœº
    action = np.random.randint(0, env.action_num)
    feature, _, alive = env.next(action)

    step = 0

    print(f"\nğŸ¬ å¼€å§‹å¯è§†åŒ–...\n")

    while alive and step < max_steps:
        step += 1

        print(f"  Processing Step {step:3d}...", end='')

        # å¯è§†åŒ–å¹¶è·å–æœ€ä½³åŠ¨ä½œ
        action = visualize_step_rewards(env, agent, step, output_dir)

        # æ‰§è¡ŒåŠ¨ä½œ
        feature, reward, alive = env.next(action)

        print(f" Score={env.game.score:4d}, Fruits={len(env.game.fruits):2d}, Action={action:2d}")

    print(f"\n{'='*70}")
    print("âœ… å¯è§†åŒ–å®Œæˆ!")
    print(f"{'='*70}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  æœ€ç»ˆå¾—åˆ†: {env.game.score}")
    print(f"  æ€»æ­¥æ•°: {step}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {step} å¼ å›¾ç‰‡")
    print(f"\nğŸ“ æ‰€æœ‰å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_dir}/")
    print("="*70)

    # åˆ›å»ºæ±‡æ€»è§†é¢‘
    create_summary_video(output_dir, step)

    return env.game.score, step


def create_summary_video(output_dir, total_steps):
    """å°†æ‰€æœ‰å¯è§†åŒ–å›¾ç‰‡åˆæˆè§†é¢‘"""

    print(f"\nğŸ¥ æ­£åœ¨åˆ›å»ºæ±‡æ€»è§†é¢‘...")

    # è¯»å–ç¬¬ä¸€å¼ å›¾ç‰‡è·å–å°ºå¯¸
    first_img_path = os.path.join(output_dir, 'step_001_rewards.png')
    if not os.path.exists(first_img_path):
        print("âš ï¸  æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶")
        return

    first_img = cv2.imread(first_img_path)
    height, width = first_img.shape[:2]

    # åˆ›å»ºè§†é¢‘ï¼ˆä½¿ç”¨æ›´é€šç”¨çš„avc1ç¼–ç å™¨ï¼‰
    video_path = os.path.join(output_dir, 'rewards_summary.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'avc1')
    video_writer = cv2.VideoWriter(video_path, fourcc, 2, (width, height))

    if not video_writer.isOpened():
        print("âš ï¸  æ— æ³•åˆ›å»ºè§†é¢‘ï¼Œå°è¯•ä½¿ç”¨mp4vç¼–ç å™¨")
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(video_path, fourcc, 2, (width, height))

    for step in range(1, total_steps + 1):
        img_path = os.path.join(output_dir, f'step_{step:03d}_rewards.png')
        if os.path.exists(img_path):
            img = cv2.imread(img_path)
            video_writer.write(img)

    video_writer.release()
    print(f"âœ… è§†é¢‘å·²ä¿å­˜: {video_path}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='å¯è§†åŒ–MCTSæ¯ä¸€æ­¥çš„reward')
    parser.add_argument('--seed', type=int, default=888, help='éšæœºç§å­')
    parser.add_argument('--sims', type=int, default=50, help='MCTSæ¨¡æ‹Ÿæ¬¡æ•°')
    parser.add_argument('--steps', type=int, default=30, help='æœ€å¤§æ­¥æ•°')
    parser.add_argument('--output', type=str, default='mcts_rewards_viz',
                       help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    run_game_with_visualization(
        seed=args.seed,
        num_sims=args.sims,
        max_steps=args.steps,
        output_dir=args.output
    )

    print("\nâœ… å®Œæˆ!")
