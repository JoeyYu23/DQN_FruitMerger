# 训练、验证、测试流程详解

## 📋 概览

本项目使用标准的机器学习三分法：**训练集（Train）、验证集（Validation）、测试集（Test）**

## 🔄 完整流程图

```
训练开始
   ↓
预热经验池 (5000条经验)
   ↓
┌─────────────────────────────────────────────┐
│ 主训练循环 (Episode 0 → 500)                 │
│                                             │
│  每个Episode:                                │
│  ├─ 训练1局 (使用随机种子)                     │
│  ├─ 更新网络参数                              │
│  ├─ 记录训练指标                              │
│  │                                          │
│  └─ 每25个Episode:                          │
│      ├─ 在验证集评估 (10局，固定种子)          │
│      ├─ 记录验证指标                          │
│      └─ 如果验证分数最佳 → 保存best_model     │
└─────────────────────────────────────────────┘
   ↓
在测试集最终评估 (10局，固定种子)
   ↓
保存final.pdparams
   ↓
训练结束
```

## 📊 三个数据集的区别

### 1️⃣ 训练集 (Training Set)

**用途：** 训练模型，更新网络参数

**频率：** 每个episode都在训练
- Episode 0, 1, 2, 3, ..., 500 共501次

**种子设置：**
```python
# 训练时使用随机种子（通过set_global_seed(42)控制随机性）
env.reset()  # 没有指定seed参数，每次都不同
```

**特点：**
- ✅ 每局游戏都不同（增加泛化能力）
- ✅ Agent在训练时会探索（ε-greedy策略）
- ✅ 每局都更新网络权重
- ❌ 不适合评估性能（因为有随机探索）

**记录内容：**
- 每个episode的分数、奖励、Loss、ε值
- 共记录501次（Episode 0-500）

---

### 2️⃣ 验证集 (Validation Set)

**用途：** 监控训练过程，选择最佳模型，防止过拟合

**频率：** 每25个episode评估一次
- Episode 0, 25, 50, 75, 100, ..., 500 共21次

**种子设置：**
```python
# 验证集使用固定种子，确保每次评估都在相同的游戏上
VAL_SEEDS = [10000, 10001, 10002, ..., 10009]  # 10个固定种子
```

**评估流程：**
```python
for i in range(10):  # 评估10局
    env.reset(seed=VAL_SEEDS[i])  # 使用固定种子
    # Agent使用纯贪婪策略（不探索）
    action = agent.predict(feature)  # 使用最优动作
    ...
```

**特点：**
- ✅ 游戏环境固定（10个固定的游戏）
- ✅ Agent不探索（纯贪婪策略）
- ✅ 可以公平对比不同训练阶段的性能
- ✅ 用于保存最佳模型（best_model.pdparams）
- ❌ 不能用于最终性能评估（因为模型见过这些游戏）

**记录内容：**
- 10局的平均分数、标准差、平均奖励
- 共记录21次（每25个episode一次）

**最佳模型保存：**
```python
if val_score > best_val_score:
    save("weights/best_model.pdparams")  # 保存验证集上最好的模型
```

---

### 3️⃣ 测试集 (Test Set)

**用途：** 最终性能评估，报告模型的真实泛化能力

**频率：** 只在训练完成后评估1次

**种子设置：**
```python
# 测试集使用与验证集完全不同的固定种子
TEST_SEEDS = [20000, 20001, 20002, ..., 20009]  # 10个不同的固定种子
```

**评估流程：**
```python
# 训练完成后
for i in range(10):  # 评估10局
    env.reset(seed=TEST_SEEDS[i])  # 使用测试集的固定种子
    action = agent.predict(feature)  # 纯贪婪策略
    ...
```

**特点：**
- ✅ 游戏环境固定但与验证集不同
- ✅ 模型从未见过这些游戏
- ✅ 评估真实的泛化能力
- ✅ 用于最终报告性能

**记录内容：**
- 10局的平均分数、标准差、平均奖励
- 只记录1次（训练结束时）

---

## 📈 实际训练数据统计

以500轮训练为例：

| 数据集 | 评估次数 | 每次局数 | 总局数 | 种子范围 |
|--------|---------|---------|--------|---------|
| **训练集** | 501次 | 1局/次 | 501局 | 随机（受全局种子42控制） |
| **验证集** | 21次 | 10局/次 | 210局 | 固定 [10000-10009] |
| **测试集** | 1次 | 10局/次 | 10局 | 固定 [20000-20009] |
| **总计** | - | - | **721局** | - |

---

## 🎯 为什么这样设计？

### ✅ 训练集用随机种子
- 让模型见到各种不同的游戏状态
- 增强泛化能力
- 避免过拟合到特定的游戏序列

### ✅ 验证集用固定种子
- 公平对比不同训练阶段的性能
- 监控训练进度（是在进步还是过拟合？）
- 选择最佳模型（哪个checkpoint性能最好？）

### ✅ 测试集用不同的固定种子
- 评估在未见过的游戏上的真实性能
- 避免"作弊"（模型可能记住了验证集）
- 提供可靠的最终性能报告

---

## 📊 可视化图表中的体现

在生成的 `training_curves.png` 中：

1. **训练分数（蓝色）**
   - 显示501个点，每个episode一个
   - 波动较大（因为游戏不同，且有探索）

2. **验证分数（红色）**
   - 显示21个点，每25个episode一个
   - 相对平滑（固定游戏，无探索）
   - 带标准差阴影（10局的波动范围）

3. **测试分数（红色虚线）**
   - 在 `val_test_comparison.png` 中显示
   - 只有一个值：175.9
   - 用于对比验证集性能

---

## 🔍 如何查看详细数据？

所有数据都保存在 `training_metrics.json` 中：

```json
{
  "config": {...},
  "training": [
    {"episode": 0, "score": 189, "reward": -910, ...},
    {"episode": 1, "score": 142, "reward": -925, ...},
    ...  // 共501条记录
  ],
  "validation": [
    {"episode": 0, "mean_score": 162.2, "std_score": 36.1, ...},
    {"episode": 25, "mean_score": 177.6, "std_score": 33.0, ...},
    ...  // 共21条记录
  ],
  "test": [
    {"episode": 500, "mean_score": 175.9, "std_score": 39.6, ...}
    // 只有1条记录
  ]
}
```

---

## 💡 常见问题

### Q1: 为什么训练集不用固定种子？
**A:** 固定种子会让模型只学会玩那几个固定的游戏，无法泛化到新游戏。

### Q2: 验证集见过的游戏，会不会导致模型"作弊"？
**A:** DQN是off-policy算法，验证时只是用网络预测，不更新参数。但确实可能过拟合验证集，这就是为什么需要测试集。

### Q3: 能不能只用训练集和测试集，不用验证集？
**A:** 可以，但你会失去：
- 无法监控训练进度
- 不知道何时停止训练
- 不知道哪个checkpoint最好
- 可能过拟合或欠拟合

### Q4: 测试集为什么只评估1次？
**A:** 测试集应该只在最后用一次，如果多次使用并调参，它就变成了验证集。

### Q5: 如何增加评估频率？
修改配置：
```python
EVAL_INTERVAL = 10  # 每10轮评估一次（原来是25）
EVAL_EPISODES = 20  # 每次评估20局（原来是10）
```

---

## 🚀 推荐实践

1. **开发阶段：** 使用500轮快速训练，观察曲线
2. **正式训练：** 使用2000-5000轮，获得更好性能
3. **超参数调优：** 只看验证集，不看测试集
4. **最终报告：** 用测试集的性能作为最终结果

---

*生成于 2025-11-18 by Claude Code*
