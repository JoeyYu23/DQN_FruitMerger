"""
åˆ†æé«˜åˆ†å±€ - å¯è§†åŒ–AIçš„å†³ç­–è¿‡ç¨‹
"""
import numpy as np
import paddle
import cv2
from DQN import Agent, build_model
from GameInterface import GameInterface
from Game import visualize_feature
from render_utils import cover
import time

def replay_game_with_analysis(agent, env, seed, save_video=False):
    """é‡æ”¾æ¸¸æˆå¹¶åˆ†ææ¯ä¸€æ­¥"""
    env.reset(seed=seed)

    decisions = []  # è®°å½•æ¯ä¸€æ­¥çš„å†³ç­–

    action = np.random.randint(0, env.action_num)
    feature, _, alive = env.next(action)

    step_count = 0
    reward_sum = 0

    print(f"\n{'='*60}")
    print(f"é‡æ”¾æ¸¸æˆ (seed={seed})")
    print(f"{'='*60}")

    while alive:
        step_count += 1

        # è·å–æ‰€æœ‰åŠ¨ä½œçš„Qå€¼
        with paddle.no_grad():
            q_values = agent.policy_net(paddle.to_tensor(feature)).numpy()

        # é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        action = agent.predict(feature)
        if isinstance(action, np.ndarray):
            action = action.item()

        # è®°å½•å†³ç­–ä¿¡æ¯
        decision = {
            'step': step_count,
            'action': action,
            'q_values': q_values.copy(),
            'best_q': q_values[action],
            'current_fruit': env.game.current_fruit_type,
            'feature': feature.copy(),
            'score_before': env.game.score
        }

        # æ‰§è¡ŒåŠ¨ä½œ
        next_feature, reward, alive = env.next(action)
        reward_sum += np.sum(reward)

        decision['reward'] = np.sum(reward)
        decision['score_after'] = env.game.score
        decision['alive'] = alive

        decisions.append(decision)

        # æ‰“å°å…³é”®æ­¥éª¤
        if np.sum(reward) > 0:  # æœ‰æ­£å¥–åŠ±ï¼ˆåˆæˆäº†ï¼‰
            fruit_name = ['', 'è‘¡è„', 'æ¨±æ¡ƒ', 'è‰è“', 'æ©™å­', 'æŸ¿å­', 'æ¡ƒå­', 'è è', 'æ¤°å­', 'è¥¿ç“œåŠ', 'è¥¿ç“œ', 'å¤§è¥¿ç“œ'][decision['current_fruit']]
            print(f"  æ­¥éª¤ {step_count:3d}: æ”¾ç½®{fruit_name} åœ¨ä½ç½®{action:2d} "
                  f"â†’ å¥–åŠ±={reward:5.1f}, åˆ†æ•°={env.game.score:3d}, "
                  f"Qå€¼={decision['best_q']:.2f}")

        feature = next_feature

    print(f"\næœ€ç»ˆç»“æœ: åˆ†æ•°={env.game.score}, æ€»å¥–åŠ±={reward_sum:.1f}, æ­¥æ•°={step_count}")

    return decisions, env.game.score, reward_sum

def analyze_decisions(decisions):
    """åˆ†æå†³ç­–æ¨¡å¼"""
    print(f"\n{'='*60}")
    print("å†³ç­–åˆ†æ")
    print(f"{'='*60}")

    # åŠ¨ä½œåˆ†å¸ƒ
    actions = [d['action'] for d in decisions]
    action_counts = np.bincount(actions, minlength=16)

    print("\nğŸ“Š åŠ¨ä½œä½ç½®åˆ†å¸ƒ:")
    for i in range(16):
        if action_counts[i] > 0:
            bar = 'â–ˆ' * int(action_counts[i] / max(action_counts) * 30)
            print(f"  ä½ç½® {i:2d}: {action_counts[i]:3d}æ¬¡ {bar}")

    # æ‰¾å‡ºæœ€å¸¸ç”¨çš„ä½ç½®
    top_positions = np.argsort(action_counts)[::-1][:3]
    print(f"\nğŸ¯ æœ€å¸¸ç”¨ä½ç½®: {', '.join([str(p) for p in top_positions if action_counts[p] > 0])}")

    # Qå€¼ç»Ÿè®¡
    q_values = [d['best_q'] for d in decisions]
    print(f"\nğŸ“ˆ Qå€¼ç»Ÿè®¡:")
    print(f"  å¹³å‡Qå€¼: {np.mean(q_values):.2f}")
    print(f"  æœ€é«˜Qå€¼: {np.max(q_values):.2f}")
    print(f"  æœ€ä½Qå€¼: {np.min(q_values):.2f}")

    # å¥–åŠ±åˆ†å¸ƒ
    rewards = [d['reward'] for d in decisions if d['reward'] > 0]
    if rewards:
        print(f"\nğŸ’° æ­£å¥–åŠ±ç»Ÿè®¡:")
        print(f"  è·å¾—å¥–åŠ±æ¬¡æ•°: {len(rewards)}")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}")
        print(f"  æœ€é«˜å¥–åŠ±: {np.max(rewards):.0f}")

    # æ‰¾å‡ºå…³é”®å†³ç­–ï¼ˆé«˜å¥–åŠ±ï¼‰
    high_reward_steps = [d for d in decisions if d['reward'] >= 10]
    if high_reward_steps:
        print(f"\nğŸŒŸ å…³é”®å†³ç­–ï¼ˆå¥–åŠ±â‰¥10ï¼‰:")
        for d in high_reward_steps[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  æ­¥éª¤ {d['step']:3d}: ä½ç½®{d['action']:2d}, "
                  f"å¥–åŠ±={d['reward']:.0f}, Qå€¼={d['best_q']:.2f}")

def visualize_game_step(agent, env, seed, target_step, action_dim):
    """å¯è§†åŒ–ç‰¹å®šæ­¥éª¤çš„æ¸¸æˆçŠ¶æ€å’ŒAIå†³ç­–"""
    env.reset(seed=seed)

    action = np.random.randint(0, env.action_num)
    feature, _, alive = env.next(action)

    step = 0

    while alive and step < target_step:
        step += 1
        action = agent.predict(feature)
        if isinstance(action, np.ndarray):
            action = action.item()
        feature, _, alive = env.next(action)

    if not alive or step != target_step:
        print(f"æ— æ³•åˆ°è¾¾æ­¥éª¤ {target_step}")
        return

    # è·å–Qå€¼
    with paddle.no_grad():
        q_values = agent.policy_net(paddle.to_tensor(feature)).numpy().flatten()

    # å¯è§†åŒ–
    feature_map_height = GameInterface.FEATURE_MAP_HEIGHT
    feature_map_width = GameInterface.FEATURE_MAP_WIDTH

    # ç»˜åˆ¶æ¸¸æˆç”»é¢
    screen = env.game.draw()

    # ç»˜åˆ¶æ‰€æœ‰åŠ¨ä½œçš„Qå€¼ï¼ˆçƒ­åŠ›å›¾ï¼‰
    unit_w = 1.0 * env.game.width / action_dim

    # å½’ä¸€åŒ–Qå€¼ç”¨äºé¢œè‰²æ˜ å°„
    q_min, q_max = q_values.min(), q_values.max()
    q_norm = (q_values - q_min) / (q_max - q_min + 1e-8)

    for i in range(action_dim):
        color_intensity = int(q_norm[i] * 255)
        rect = np.zeros_like(screen, dtype=np.uint8)
        # ç»¿è‰²->é»„è‰²->çº¢è‰² è¡¨ç¤ºQå€¼ä»ä½åˆ°é«˜
        if q_norm[i] < 0.5:
            color = (0, int(255 * q_norm[i] * 2), int(255 * (1 - q_norm[i] * 2)), 100)
        else:
            color = (0, 255, 0, 100)

        cv2.rectangle(rect,
                     (int(i * unit_w), 0),
                     (int((i + 1) * unit_w), env.game.height),
                     color, -1)
        cover(screen, rect, 0.3)

    # æ ‡è®°æœ€ä½³åŠ¨ä½œ
    best_action = np.argmax(q_values)
    best_rect = np.zeros_like(screen, dtype=np.uint8)
    cv2.rectangle(best_rect,
                 (int(best_action * unit_w), 0),
                 (int((best_action + 1) * unit_w), env.game.height),
                 (0, 0, 255, 150), 3)
    cover(screen, best_rect, 1)

    # æ·»åŠ Qå€¼æ–‡æœ¬
    for i in range(action_dim):
        x = int((i + 0.5) * unit_w)
        cv2.putText(screen, f"{q_values[i]:.1f}",
                   (x - 15, 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.3,
                   (255, 255, 255), 1)

    # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
    info = f"Step:{step} Score:{env.game.score} Best:{best_action} Q:{q_values[best_action]:.2f}"
    cv2.putText(screen, info, (5, env.game.height - 5),
               cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)

    # æ˜¾ç¤ºç‰¹å¾å›¾
    reshaped_feature = feature.reshape((feature_map_height, feature_map_width, 2))
    feature_img = visualize_feature(reshaped_feature, env.game.resolution).astype(np.uint8)

    # åˆå¹¶æ˜¾ç¤º
    combined = np.hstack([cv2.cvtColor(screen, cv2.COLOR_BGRA2BGR), feature_img])

    return combined, q_values, best_action

def find_top_games(num_games=100, top_k=3):
    """æ‰¾å‡ºåˆ†æ•°æœ€é«˜çš„å‡ å±€æ¸¸æˆ"""
    print("ğŸ” å¯»æ‰¾é«˜åˆ†å±€...")

    feature_map_height = GameInterface.FEATURE_MAP_HEIGHT
    feature_map_width = GameInterface.FEATURE_MAP_WIDTH
    action_dim = GameInterface.ACTION_NUM
    feature_dim = feature_map_height * feature_map_width * 2

    env = GameInterface()
    agent = Agent(build_model, feature_dim, action_dim, e_greed=0.0)
    agent.policy_net.set_state_dict(paddle.load("final.pdparams"))

    game_results = []

    for seed in range(num_games):
        env.reset(seed=seed)
        step_count = 0
        reward_sum = 0

        action = np.random.randint(0, env.action_num)
        feature, _, alive = env.next(action)

        while alive:
            step_count += 1
            action = agent.predict(feature)
            if isinstance(action, np.ndarray):
                action = action.item()
            feature, reward, alive = env.next(action)
            reward_sum += np.sum(reward)

        game_results.append({
            'seed': seed,
            'score': env.game.score,
            'reward': reward_sum,
            'steps': step_count
        })

        if (seed + 1) % 20 == 0:
            print(f"  å·²æ‰«æ {seed + 1}/{num_games} å±€")

    # æ’åºæ‰¾å‡ºå‰kå
    game_results.sort(key=lambda x: x['score'], reverse=True)

    print(f"\nğŸ† Top {top_k} é«˜åˆ†å±€:")
    for i, game in enumerate(game_results[:top_k]):
        print(f"  #{i+1}: Seed={game['seed']:3d}, "
              f"åˆ†æ•°={game['score']:3d}, "
              f"å¥–åŠ±={game['reward']:6.1f}, "
              f"æ­¥æ•°={game['steps']:3d}")

    return game_results[:top_k], agent, env

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”¬ DQNé«˜åˆ†å±€åˆ†æ")
    print("=" * 60)

    # æ‰¾å‡ºé«˜åˆ†å±€
    top_games, agent, env = find_top_games(num_games=100, top_k=5)

    # è¯¦ç»†åˆ†ææœ€é«˜åˆ†å±€
    best_game = top_games[0]
    print(f"\n\n{'='*60}")
    print(f"ğŸ¯ è¯¦ç»†åˆ†ææœ€é«˜åˆ†å±€ (Seed={best_game['seed']})")
    print(f"{'='*60}")

    decisions, final_score, total_reward = replay_game_with_analysis(
        agent, env, best_game['seed']
    )

    analyze_decisions(decisions)

    # å¯è§†åŒ–å…³é”®æ­¥éª¤
    print(f"\n\n{'='*60}")
    print("ğŸ“¸ ç”Ÿæˆå…³é”®æ­¥éª¤å¯è§†åŒ–")
    print(f"{'='*60}")

    key_steps = [1, len(decisions)//4, len(decisions)//2, len(decisions)*3//4, len(decisions)-1]

    for step in key_steps:
        if step < len(decisions):
            img, q_values, best_action = visualize_game_step(
                agent, env, best_game['seed'], step, GameInterface.ACTION_NUM
            )
            filename = f"high_score_step_{step:03d}.png"
            cv2.imwrite(filename, img)
            print(f"  ä¿å­˜æ­¥éª¤ {step:3d}: {filename}")

    print(f"\nâœ… åˆ†æå®Œæˆ!")
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    print(f"  - æœ€é«˜åˆ†: {best_game['score']} (Seed {best_game['seed']})")
    print(f"  - å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
    print(f"  - æŸ¥çœ‹å›¾ç‰‡äº†è§£AIçš„å†³ç­–è¿‡ç¨‹")
