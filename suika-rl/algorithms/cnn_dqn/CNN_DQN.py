"""
CNN-based DQN for Suika Game (PyTorch version)
æ”¹è¿›ï¼šä½¿ç”¨CNNæå–ç©ºé—´ç‰¹å¾ï¼Œæ›¿ä»£MLPçš„flattenè¾“å…¥
ä¿®æ­£ï¼šGameInterfaceè¿”å›(640,)éœ€è¦reshapeåˆ°(20,16,2)
"""
import os
import random
import collections
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================
# è¶…å‚æ•°
# ============================================
MEMORY_SIZE = 50000
MEMORY_WARMUP_SIZE = 5000
BATCH_SIZE = 32
LEARNING_RATE = 0.00005  # ğŸ”§ é™ä½å­¦ä¹ ç‡æé«˜ç¨³å®šæ€§ (0.0001 â†’ 0.00005)
GAMMA = 0.99
TARGET_UPDATE = 200

# ============================================
# CNN Q-Network
# ============================================
class CNN_QNet(nn.Module):
    """
    CNN-based Q-Network for Suika Game

    Input: (batch, 2, 20, 16) - 2é€šé“çš„20Ã—16ç‰¹å¾å›¾
    Output: (batch, 16) - 16ä¸ªåŠ¨ä½œçš„Qå€¼
    """
    def __init__(self, action_dim=16):
        super(CNN_QNet, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(2, 16, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(16)

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(32)

        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(64)

        # MaxPooling (optional, å‡å°‘ç»´åº¦)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # è®¡ç®—flattenåçš„ç»´åº¦
        # After conv: (2, 20, 16)
        # After pool1: (16, 10, 8)
        # After pool2: (32, 5, 4)
        # After pool3: (64, 5, 4) â†’ no pool
        # Flatten: 64 * 5 * 4 = 1280
        self.fc1 = nn.Linear(64 * 5 * 4, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, action_dim)

        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x shape: (batch, 2, 20, 16)

        # Conv block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool(x)  # â†’ (batch, 16, 10, 8)

        # Conv block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool(x)  # â†’ (batch, 32, 5, 4)

        # Conv block 3 (no pool to preserve info)
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)  # â†’ (batch, 64, 5, 4)

        # Flatten
        x = x.view(x.size(0), -1)  # â†’ (batch, 1280)

        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)  # â†’ (batch, 16)

        return x


# ============================================
# Replay Buffer
# ============================================
class ReplayMemory:
    def __init__(self, max_size=MEMORY_SIZE):
        self.buffer = collections.deque(maxlen=max_size)

    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ä¸€æ¡ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        """é‡‡æ ·batch"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        states = np.array(states, dtype=np.float32)      # (batch, 640)
        actions = np.array(actions, dtype=np.int64)
        rewards = np.array(rewards, dtype=np.float32)
        next_states = np.array(next_states, dtype=np.float32)
        dones = np.array(dones, dtype=np.float32)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)


# ============================================
# CNN DQN Agent
# ============================================
class CNN_DQN_Agent:
    def __init__(self, action_dim=16, learning_rate=LEARNING_RATE,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.action_dim = action_dim
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # åˆ›å»ºQç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.policy_net = CNN_QNet(action_dim).to(device)
        self.target_net = CNN_QNet(action_dim).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)

        # è®¡æ•°å™¨
        self.global_step = 0
        self.target_update = TARGET_UPDATE

    def preprocess_state(self, state):
        """
        é¢„å¤„ç†çŠ¶æ€ï¼š(640,) â†’ (20, 16, 2) â†’ (2, 20, 16)

        Args:
            state: numpy array of shape (640,) or (batch, 640)

        Returns:
            torch tensor of shape (2, 20, 16) or (batch, 2, 20, 16)
        """
        if state.ndim == 1:  # Single state (640,)
            # (640,) â†’ (20, 16, 2)
            state = state.reshape(20, 16, 2)
            # (20, 16, 2) â†’ (2, 20, 16)
            state = np.transpose(state, (2, 0, 1))
            state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dim
        else:  # Batch of states (batch, 640)
            # (batch, 640) â†’ (batch, 20, 16, 2)
            state = state.reshape(-1, 20, 16, 2)
            # (batch, 20, 16, 2) â†’ (batch, 2, 20, 16)
            state = np.transpose(state, (0, 3, 1, 2))
            state = torch.FloatTensor(state).to(device)

        return state

    def select_action(self, state, training=True):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰

        Args:
            state: numpy array (640,)
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼

        Returns:
            action: int
        """
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)

        with torch.no_grad():
            state_tensor = self.preprocess_state(state)
            q_values = self.policy_net(state_tensor)
            action = q_values.argmax().item()

        return action

    def learn(self, memory, batch_size=BATCH_SIZE):
        """ä»replay bufferå­¦ä¹  (with numerical stability)"""
        if len(memory) < batch_size:
            return None

        # é‡‡æ ·batch
        states, actions, rewards, next_states, dones = memory.sample(batch_size)

        # é¢„å¤„ç†çŠ¶æ€ (batch, 640) â†’ (batch, 2, 20, 16)
        states = self.preprocess_state(states)
        next_states = self.preprocess_state(next_states)

        actions = torch.LongTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).to(device)
        dones = torch.FloatTensor(dones).to(device)

        # ğŸ”§ å½’ä¸€åŒ–reward (é˜²æ­¢Qå€¼çˆ†ç‚¸)
        rewards = rewards / 100.0

        # è®¡ç®—å½“å‰Qå€¼
        q_values = self.policy_net(states)
        # ğŸ”§ è£å‰ªQå€¼åˆ°åˆç†èŒƒå›´
        q_values = torch.clamp(q_values, -10, 10)
        current_q = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)

        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_net(next_states)
            # ğŸ”§ è£å‰ªç›®æ ‡ç½‘ç»œçš„Qå€¼
            next_q_values = torch.clamp(next_q_values, -10, 10)
            next_q = next_q_values.max(1)[0]
            target_q = rewards + GAMMA * next_q * (1 - dones)
            # ğŸ”§ è£å‰ªç›®æ ‡Qå€¼
            target_q = torch.clamp(target_q, -10, 10)

        # ğŸ”§ ä½¿ç”¨Huber lossæ›¿ä»£MSE (å¯¹å¼‚å¸¸å€¼æ›´é²æ£’)
        loss = F.smooth_l1_loss(current_q, target_q)

        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        # ğŸ”§ æ›´å¼ºçš„æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()

        # æ›´æ–°target network
        self.global_step += 1
        if self.global_step % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        return loss.item()

    def decay_epsilon(self):
        """è¡°å‡epsilon"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'global_step': self.global_step
        }, path)
        print(f"Model saved to {path}")

    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=device)
        self.policy_net.load_state_dict(checkpoint['policy_net'])
        self.target_net.load_state_dict(checkpoint['target_net'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']
        self.global_step = checkpoint['global_step']
        print(f"Model loaded from {path}")


# ============================================
# è®­ç»ƒå‡½æ•°
# ============================================
def train_cnn_dqn(env, agent, memory, num_episodes=2000,
                  save_interval=500, model_dir="weights_cnn_dqn"):
    """
    è®­ç»ƒCNN-DQN

    Args:
        env: æ¸¸æˆç¯å¢ƒ (GameInterface)
        agent: CNN_DQN_Agent
        memory: ReplayMemory
        num_episodes: è®­ç»ƒè½®æ•°
        save_interval: ä¿å­˜é—´éš”
        model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
    """
    os.makedirs(model_dir, exist_ok=True)

    print("="*60)
    print("Training CNN-DQN")
    print("="*60)
    print(f"Device: {device}")
    print(f"Episodes: {num_episodes}")
    print(f"Memory size: {MEMORY_SIZE}")
    print(f"Batch size: {BATCH_SIZE}")
    print("="*60)

    # Warmup: å¡«å……replay buffer
    print("\nWarming up replay buffer...")
    while len(memory) < MEMORY_WARMUP_SIZE:
        env.reset()
        action = random.randint(0, agent.action_dim - 1)
        state, _, alive = env.next(action)

        while alive:
            action = random.randint(0, agent.action_dim - 1)
            next_state, reward, alive = env.next(action)
            memory.push(state, action, reward, next_state, not alive)
            state = next_state

        if len(memory) % 500 == 0:
            print(f"  Buffer size: {len(memory)}/{MEMORY_WARMUP_SIZE}")

    print(f"Warmup complete! Buffer size: {len(memory)}\n")

    # è®­ç»ƒå¾ªç¯
    scores = []
    losses = []

    for episode in range(num_episodes):
        env.reset()
        action = random.randint(0, agent.action_dim - 1)
        state, _, alive = env.next(action)

        episode_reward = 0
        episode_loss = []
        steps = 0

        while alive:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, alive = env.next(action)

            # å­˜å‚¨ç»éªŒ
            memory.push(state, action, reward, next_state, not alive)

            # å­¦ä¹ 
            loss = agent.learn(memory)
            if loss is not None:
                episode_loss.append(loss)

            episode_reward += np.sum(reward) if hasattr(reward, '__len__') else reward
            state = next_state
            steps += 1

        # è¡°å‡epsilon
        agent.decay_epsilon()

        # è®°å½•
        score = env.game.score
        scores.append(score)
        if episode_loss:
            losses.append(np.mean(episode_loss))

        # æ‰“å°è¿›åº¦
        if (episode + 1) % 50 == 0:
            avg_score = np.mean(scores[-50:])
            avg_loss = np.mean(losses[-50:]) if losses else 0
            print(f"Episode {episode+1:4d} | "
                  f"Score: {score:3d} | "
                  f"Avg Score: {avg_score:6.1f} | "
                  f"Avg Loss: {avg_loss:.4f} | "
                  f"Îµ: {agent.epsilon:.3f} | "
                  f"Steps: {steps:3d}")

        # ä¿å­˜æ¨¡å‹
        if (episode + 1) % save_interval == 0:
            save_path = os.path.join(model_dir, f"checkpoint_ep{episode+1}.pth")
            agent.save(save_path)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(model_dir, "final_model.pth")
    agent.save(final_path)

    print("\n" + "="*60)
    print("Training Complete!")
    print(f"Final average score: {np.mean(scores[-100:]):.1f}")
    print("="*60)

    return scores, losses


# ============================================
# æµ‹è¯•å‡½æ•°
# ============================================
def test_cnn_dqn(env, agent, num_episodes=10):
    """æµ‹è¯•CNN-DQN"""
    agent.policy_net.eval()
    scores = []

    for episode in range(num_episodes):
        env.reset(seed=1000 + episode)
        action = random.randint(0, agent.action_dim - 1)
        state, _, alive = env.next(action)

        while alive:
            action = agent.select_action(state, training=False)
            state, _, alive = env.next(action)

        score = env.game.score
        scores.append(score)
        print(f"Episode {episode+1}: Score = {score}")

    print(f"\nTest Results:")
    print(f"  Mean: {np.mean(scores):.1f} Â± {np.std(scores):.1f}")
    print(f"  Max: {max(scores)}")
    print(f"  Min: {min(scores)}")

    return scores


# ============================================
# ä¸»å‡½æ•°
# ============================================
if __name__ == "__main__":
    # å¯¼å…¥GameInterface
    from GameInterface import GameInterface

    print("Initializing...")
    print(f"PyTorch version: {torch.__version__}")
    print(f"Device: {device}")

    # åˆ›å»ºç¯å¢ƒå’Œagent
    env = GameInterface()
    agent = CNN_DQN_Agent(action_dim=16)
    memory = ReplayMemory()

    # æµ‹è¯•ä¸€æ­¥ç¡®è®¤çŠ¶æ€å½¢çŠ¶
    env.reset()
    state, _, _ = env.next(0)
    print(f"GameInterface state shape: {state.shape}")
    print(f"After preprocessing: {agent.preprocess_state(state).shape}\n")

    # è®­ç»ƒ
    scores, losses = train_cnn_dqn(env, agent, memory, num_episodes=2000)

    # æµ‹è¯•
    test_cnn_dqn(env, agent, num_episodes=10)
