# ğŸ“ AlphaZeroè®­ç»ƒæµç¨‹å®Œå…¨è§£æ

## ğŸ¯ æ€»ä½“æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å¼€å§‹è®­ç»ƒ                                    â”‚
â”‚              TrainAlphaZero.py                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚  åˆå§‹åŒ–: SuikaNet (éšæœºæƒé‡)
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è¿­ä»£å¾ªç¯å¼€å§‹                              â”‚
â”‚             for iteration in range(20):                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: Self-Play (è‡ªæˆ‘åšå¼ˆ) - 30-60åˆ†é’Ÿ                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  ç›®æ ‡: æ”¶é›†è®­ç»ƒæ•°æ®                                         â”‚
â”‚  ç©50å±€æ¸¸æˆï¼Œæ¯å±€ä½¿ç”¨MCTS+ç½‘ç»œå†³ç­–                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚  è¾“å‡º: 871ä¸ªæ ·æœ¬ (state, Ï€, z)
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: Train (è®­ç»ƒç½‘ç»œ) - 5-10åˆ†é’Ÿ                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  ç›®æ ‡: è®©ç½‘ç»œå­¦ä¹ MCTSçš„ç­–ç•¥                                 â”‚
â”‚  5ä¸ªepochï¼Œæ‰¹é‡å¤§å°32                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚  è¾“å‡º: æ›´æ–°åçš„ç½‘ç»œæƒé‡
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: Evaluate (è¯„ä¼°) - 10-15åˆ†é’Ÿ                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  ç›®æ ‡: æ£€éªŒç½‘ç»œæ€§èƒ½                                         â”‚
â”‚  ç©5å±€æ¸¸æˆæµ‹è¯•                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚  è¾“å‡º: å¹³å‡å¾—åˆ†
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ4: Save (ä¿å­˜) - 1ç§’                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  ä¿å­˜: weights/alphazero/iter_X.pdparams                    â”‚
â”‚  ä¿å­˜: history.json (è®­ç»ƒå†å²)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚  æ˜¯å¦å®Œæˆæ‰€æœ‰è¿­ä»£?
             â”‚
             â”œâ”€ No  â”€â†’ å›åˆ°è¿­ä»£å¾ªç¯å¼€å§‹
             â”‚
             â†“ Yes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è®­ç»ƒå®Œæˆï¼                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ è¯¦ç»†æµç¨‹æ‹†è§£

### ğŸ® é˜¶æ®µ1: Self-Play (è‡ªæˆ‘åšå¼ˆ)

**ç›®æ ‡**: è®©å½“å‰ç½‘ç»œå’Œè‡ªå·±ç©ï¼Œæ”¶é›†è®­ç»ƒæ•°æ®

#### å•å±€æ¸¸æˆæµç¨‹:

```python
# SelfPlay.py - play_one_episode()

game = GameInterface()
game.reset()

states = []   # ä¿å­˜æ¯ä¸€æ­¥çš„çŠ¶æ€
pis = []      # ä¿å­˜æ¯ä¸€æ­¥çš„MCTSç­–ç•¥
step = 0

while game.alive:
    step += 1

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 1. è·å–å½“å‰çŠ¶æ€                    â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    state = converter.game_to_simplified(game)
    # state = SimplifiedGameState
    #   - grid: [16, 16] æ°´æœç½‘æ ¼
    #   - current_fruit: 1-10
    #   - score: å½“å‰åˆ†æ•°

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 2. æ¸©åº¦æ§åˆ¶                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    if step <= 30:
        temperature = 1.0    # å‰30æ­¥: æ¢ç´¢ï¼Œéšæœºæ€§é«˜
    else:
        temperature = 0.0    # 30æ­¥å: åˆ©ç”¨ï¼Œç¡®å®šæ€§é€‰æ‹©

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 3. MCTSæœç´¢ (æ ¸å¿ƒï¼)               â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    mcts = AlphaZeroMCTS(network, num_simulations=100)
    pi = mcts.search(state)
    # pi = [p0, p1, ..., p15]  (16ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ)

    # MCTSå†…éƒ¨æµç¨‹:
    # for _ in range(100):  # 100æ¬¡æ¨¡æ‹Ÿ
    #     1. Selection: é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹ (PUCT)
    #     2. Expansion: ç”¨ç½‘ç»œè¯„ä¼° (P, V) = network(state)
    #     3. Backup: åå‘ä¼ æ’­V
    #
    # è¿”å›: è®¿é—®æ¬¡æ•°çš„æ¦‚ç‡åˆ†å¸ƒ

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 4. é‡‡æ ·åŠ¨ä½œ                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    if temperature == 0:
        action = argmax(pi)          # ç¡®å®šæ€§
    else:
        action = sample(pi)          # éšæœºé‡‡æ ·

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 5. æ‰§è¡ŒåŠ¨ä½œ                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    game.next(action)

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 6. ä¿å­˜æ•°æ®                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    states.append(state_tensor)  # [13, 16, 16]
    pis.append(pi)               # [16]

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 7. æ¸¸æˆç»“æŸï¼Œè®¡ç®—z                 â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
final_score = game.score  # æ¯”å¦‚: 126
z = normalize_score(126)  # å½’ä¸€åŒ–åˆ° [-1, 1]

# å‡è®¾å½’ä¸€åŒ–å…¬å¼: (score - 100) / 200
z = (126 - 100) / 200 = 0.13

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ 8. æ„å»ºè®­ç»ƒæ•°æ®                    â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
training_data = []
for state, pi in zip(states, pis):
    training_data.append((state, pi, z))
    # æ¯ä¸ªæ ·æœ¬åŒ…å«:
    # - state: [13, 16, 16] çŠ¶æ€ç‰¹å¾
    # - pi: [16] MCTSå¢å¼ºçš„ç­–ç•¥
    # - z: 0.13 æœ€ç»ˆå¾—åˆ†(æ‰€æœ‰æ­¥éƒ½ç”¨åŒä¸€ä¸ªz)

return training_data
```

#### æ‰¹é‡æ”¶é›†:

```python
# SelfPlay.py - collect_episodes()

collector = SelfPlayCollector(network, num_simulations=100)

all_data = []
for episode in range(50):  # ç©50å±€
    data = play_one_episode(seed=episode)
    all_data.extend(data)
    # æ¯å±€çº¦40æ­¥ â†’ 40ä¸ªæ ·æœ¬
    # 50å±€ Ã— 40æ­¥ = 2000ä¸ªæ ·æœ¬

print(f"Collected {len(all_data)} samples")
# è¾“å‡º: Collected 871 samples
```

**å®é™…ä½ çš„è®­ç»ƒæ—¥å¿—ï¼š**
```
[SelfPlay] Collection finished!
  Total episodes: 20
  Total samples: 871
  Avg steps per episode: 43.5
  Avg score: 126.0 Â± 32.4
```

---

### ğŸ§  é˜¶æ®µ2: Train (è®­ç»ƒç½‘ç»œ)

**ç›®æ ‡**: è®©ç½‘ç»œæ¨¡ä»¿MCTSçš„å†³ç­–

#### æ•°æ®æ ¼å¼:

```python
# ä»Self-Playå¾—åˆ°çš„æ•°æ®
data = [
    (state_0, pi_0, z),  # ç¬¬1æ­¥
    (state_1, pi_1, z),  # ç¬¬2æ­¥
    ...
    (state_870, pi_870, z)  # ç¬¬871æ­¥
]

# æ¯ä¸ªæ ·æœ¬:
# state: [13, 16, 16] numpyæ•°ç»„
# pi: [16] MCTSç­–ç•¥ (ç›®æ ‡)
# z: 0.13 æœ€ç»ˆå¾—åˆ† (ç›®æ ‡)
```

#### è®­ç»ƒå¾ªç¯:

```python
# TrainAlphaZero.py - train_on_data()

dataset = SuikaDataset(data)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for epoch in range(5):  # 5ä¸ªepoch
    for states, pis, zs in dataloader:
        # states: [32, 13, 16, 16]
        # pis: [32, 16]
        # zs: [32, 1]

        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚ 1. å‰å‘ä¼ æ’­                        â”‚
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        pred_policy, pred_value = network(states)
        # pred_policy: [32, 16]
        # pred_value: [32, 1]

        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚ 2. è®¡ç®—æŸå¤±                        â”‚
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        # Policy Loss: äº¤å‰ç†µ
        # ç›®æ ‡: è®©ç½‘ç»œè¾“å‡ºæ¥è¿‘MCTSçš„ç­–ç•¥
        policy_loss = -sum(pi * log(pred_policy))
        # ä¾‹: pi=[0.1, 0.3, 0.6, ...] (MCTS)
        #     pred_policy=[0.2, 0.2, 0.6, ...] (ç½‘ç»œ)
        #     è®©ç½‘ç»œå­¦ä¹ MCTSçš„åˆ†å¸ƒ

        # Value Loss: MSE
        # ç›®æ ‡: è®©ç½‘ç»œé¢„æµ‹æœ€ç»ˆå¾—åˆ†
        value_loss = (pred_value - z)^2
        # ä¾‹: pred_value=0.05, z=0.13
        #     ç½‘ç»œä½ä¼°äº†ï¼Œéœ€è¦è°ƒæ•´

        # Total Loss
        loss = policy_loss + value_loss

        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚ 3. åå‘ä¼ æ’­                        â”‚
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()

    print(f"Epoch {epoch}: Loss={loss:.4f}")
```

**å®é™…ä½ çš„è®­ç»ƒæ—¥å¿—ï¼š**
```
[2/3] Training network...
  Epoch 1/5: Loss=2.0713 (Policy=2.0695, Value=0.0018)
  Epoch 2/5: Loss=2.0086 (Policy=2.0071, Value=0.0015)
  Epoch 3/5: Loss=1.9755 (Policy=1.9741, Value=0.0015)
  Epoch 4/5: Loss=1.9383 (Policy=1.9366, Value=0.0017)
  Epoch 5/5: Loss=1.9491 (Policy=1.9476, Value=0.0015)
```

**åˆ†æï¼š**
- âœ… Lossåœ¨ä¸‹é™ (2.07 â†’ 1.95)
- âš ï¸ Value Losså¾ˆå° (0.0015)
  - è¯´æ˜ç½‘ç»œvalueé¢„æµ‹å˜åŒ–ä¸å¤§
  - å¯èƒ½å½’ä¸€åŒ–æœ‰é—®é¢˜

---

### ğŸ“Š é˜¶æ®µ3: Evaluate (è¯„ä¼°)

**ç›®æ ‡**: æµ‹è¯•ç½‘ç»œçš„å®é™…æ¸¸æˆèƒ½åŠ›

```python
# TrainAlphaZero.py - evaluate_network()

def evaluate_network(network, num_games=5):
    mcts = AlphaZeroMCTS(network, num_simulations=100, temperature=0.0)

    scores = []
    for game_id in range(num_games):
        game = GameInterface()
        game.reset(seed=1000 + game_id)  # å›ºå®šç§å­

        while game.alive:
            state = converter.game_to_simplified(game)
            action = mcts.get_action(state)  # ç¡®å®šæ€§é€‰æ‹©
            game.next(action)

        scores.append(game.score)

    return {
        'mean_score': mean(scores),
        'max_score': max(scores),
        'std_score': std(scores)
    }
```

**å®é™…ä½ çš„è®­ç»ƒæ—¥å¿—ï¼š**
```
[3/3] Evaluating network...
  Evaluation: Mean=109.0 Â± 36.9, Max=175.0
```

---

### ğŸ’¾ é˜¶æ®µ4: Save (ä¿å­˜)

```python
# ä¿å­˜ç½‘ç»œæƒé‡
paddle.save(network.state_dict(),
           'weights/alphazero/iter_7.pdparams')

# ä¿å­˜è®­ç»ƒå†å²
history = {
    'iterations': [3, 4, 5, 6, 7],
    'train_losses': [2.21, 2.09, 1.95, 1.96, 1.95],
    'eval_scores': [99.8, 94.2, 109.0, 84.8, 99.8]
}
json.dump(history, f)
```

---

## ğŸ”„ å®Œæ•´è¿­ä»£æµç¨‹ç¤ºæ„

### è¿­ä»£1:

```
ç½‘ç»œ1 (éšæœºåˆå§‹åŒ–)
  â†“
Self-Play: ç”¨ç½‘ç»œ1ç©50å±€ â†’ æ”¶é›†2000ä¸ªæ ·æœ¬
  â†“
Train: ç”¨2000æ ·æœ¬è®­ç»ƒç½‘ç»œ â†’ ç½‘ç»œ2
  â†“
Evaluate: ç”¨ç½‘ç»œ2æµ‹è¯• â†’ å¾—åˆ†: 99.8
  â†“
Save: ä¿å­˜ç½‘ç»œ2ä¸ºiter_3.pdparams
```

### è¿­ä»£2:

```
ç½‘ç»œ2 (ä¸Šè½®è®­ç»ƒçš„)
  â†“
Self-Play: ç”¨ç½‘ç»œ2ç©50å±€ â†’ æ”¶é›†1950æ ·æœ¬
  â†“
Train: ç”¨1950æ ·æœ¬è®­ç»ƒç½‘ç»œ â†’ ç½‘ç»œ3
  â†“
Evaluate: ç”¨ç½‘ç»œ3æµ‹è¯• â†’ å¾—åˆ†: 94.2
  â†“
Save: ä¿å­˜ç½‘ç»œ3ä¸ºiter_4.pdparams
```

...ä¾æ­¤ç±»æ¨

---

## ğŸ“ˆ å…³é”®æ¦‚å¿µè§£é‡Š

### 1. ä¸ºä»€ä¹ˆç”¨MCTSç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Ÿ

```
ä¼ ç»Ÿç›‘ç£å­¦ä¹ :
  éœ€è¦: (çŠ¶æ€, æ­£ç¡®åŠ¨ä½œ) æ ‡ç­¾
  é—®é¢˜: è°æ¥å‘Šè¯‰æˆ‘ä»¬"æ­£ç¡®åŠ¨ä½œ"ï¼Ÿ

AlphaZeroæ–¹æ³•:
  ç”¨MCTSæœç´¢ â†’ å¾—åˆ°å¢å¼ºç­–ç•¥Ï€
  Ï€ = MCTSæœç´¢åçš„è®¿é—®æ¬¡æ•°åˆ†å¸ƒ

  ä¼˜åŠ¿:
  1. MCTSæ¯”å•æ¬¡ç½‘ç»œé¢„æµ‹æ›´å‡†ç¡®(æœç´¢äº†100æ¬¡)
  2. Ï€æ˜¯"è½¯æ ‡ç­¾"ï¼ŒåŒ…å«äº†ä¸ç¡®å®šæ€§
  3. ç½‘ç»œå­¦ä¹ MCTS â†’ ç½‘ç»œå˜å¼º â†’ MCTSæ›´å¼º â†’ å¾ªç¯
```

### 2. ä¸ºä»€ä¹ˆæ‰€æœ‰æ­¥éƒ½ç”¨åŒä¸€ä¸ªzï¼Ÿ

```
z = æœ€ç»ˆå¾—åˆ†å½’ä¸€åŒ–

å‡è®¾æ¸¸æˆ:
  æ­¥1: æ”¾æ°´æœï¼Œå¾—0åˆ†
  æ­¥2: æ”¾æ°´æœï¼Œå¾—0åˆ†
  æ­¥3: åˆå¹¶ï¼Œå¾—3åˆ†
  æ­¥4: åˆå¹¶ï¼Œå¾—5åˆ†
  ...
  æœ€ç»ˆå¾—åˆ†: 126

è®­ç»ƒæ•°æ®:
  (state_1, pi_1, z=0.13)
  (state_2, pi_2, z=0.13)
  (state_3, pi_3, z=0.13)
  (state_4, pi_4, z=0.13)

é€»è¾‘:
  - æ¯ä¸€æ­¥çš„ä»·å€¼ = "ä»è¿™ä¸ªçŠ¶æ€èƒ½è¾¾åˆ°çš„æœ€ç»ˆå¾—åˆ†"
  - å¦‚æœæœ€ç»ˆå¾—126åˆ†ï¼Œé‚£æ¯ä¸€æ­¥éƒ½å¯¹è¿™ä¸ªç»“æœæœ‰è´¡çŒ®
  - zæ˜¯"hindsight"æ ‡ç­¾ (äº‹åè¯¸è‘›äº®)
```

### 3. æ¸©åº¦(Temperature)çš„ä½œç”¨

```python
# å‰30æ­¥: temperature = 1.0
pi = visits ** (1.0 / 1.0) = visits
# ç»“æœ: æŒ‰è®¿é—®æ¬¡æ•°æ¯”ä¾‹é‡‡æ · (æ¢ç´¢)

# ä¾‹: visits = [10, 30, 60]
#     pi = [0.1, 0.3, 0.6]
#     å¯èƒ½é€‰2ï¼Œä¹Ÿå¯èƒ½é€‰0

# 30æ­¥å: temperature = 0.0
action = argmax(visits)
# ç»“æœ: é€‰æœ€å¤šè®¿é—®çš„ (åˆ©ç”¨)

# ä¾‹: visits = [10, 30, 60]
#     action = 2 (ç¡®å®šæ€§)
```

---

## ğŸ› ä½ çš„è®­ç»ƒé—®é¢˜è¯Šæ–­

### é—®é¢˜1: åˆ†æ•°ä¸ç¨³å®š

```
iter_3: 99.8
iter_4: 94.2
iter_5: 109.0
iter_6: 84.8  â† åè€Œä¸‹é™
iter_7: 99.8
```

**åŸå› ï¼š**
1. âŒ è®­ç»ƒæ ·æœ¬å¤ªå°‘ (æ¯è½®åªç©20å±€)
2. âŒ MCTSæ¨¡æ‹Ÿæ¬¡æ•°å°‘ (100æ¬¡)
3. âŒ åŠ¨ä½œç©ºé—´æ˜ å°„é”™è¯¯ (å·²ä¿®å¤)
4. âŒ Value Losså¤ªå°ï¼Œç½‘ç»œvalueå¤´æ²¡å­¦åˆ°ä¸œè¥¿

**è§£å†³ï¼š**
- âœ… ä¿®å¤åŠ¨ä½œç©ºé—´ä¸º16 (å·²å®Œæˆ)
- ğŸ”§ ä¿®å¤ç»ˆå±€ä»·å€¼è¯„ä¼°
- ğŸ”§ å¢åŠ è®­ç»ƒå±€æ•°åˆ°50
- ğŸ”§ å¢åŠ MCTSæ¨¡æ‹Ÿåˆ°200

### é—®é¢˜2: Value Lossè¿‡å°

```
Value Loss: 0.0015 - 0.0037
```

**åŸå› ï¼š**
- zçš„å½’ä¸€åŒ–èŒƒå›´å¤ªå°
- æ‰€æœ‰zéƒ½æ¥è¿‘0

**ä¾‹å­ï¼š**
```python
# å½“å‰å½’ä¸€åŒ– (æ¨æµ‹)
def normalize_score(score):
    return (score - 100) / 200

# åˆ†æ•°èŒƒå›´: 50-200
zèŒƒå›´: (50-100)/200 = -0.25
      åˆ° (200-100)/200 = 0.5

# é—®é¢˜: èŒƒå›´å¤ªçª„ï¼Œæ¢¯åº¦å¤ªå°
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

1. **ç«‹å³æµ‹è¯•ä¿®æ”¹åçš„ä»£ç **
   ```bash
   python TrainAlphaZero.py --iterations 1 --games 5 --simulations 50
   ```

2. **ä¿®å¤ç»ˆå±€ä»·å€¼** (è§CODE_REVIEW_MCTS.md)

3. **é‡æ–°è®­ç»ƒ10-20è½®**
   ```bash
   python TrainAlphaZero.py --iterations 15 --games 30 --simulations 150
   ```

4. **å¯¹æ¯”æ–°æ—§æ¨¡å‹**

---

**ç°åœ¨æ˜ç™½è®­ç»ƒæµç¨‹äº†å—ï¼Ÿ** ğŸš€
