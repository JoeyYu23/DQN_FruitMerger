"""
Real Physics MCTS - ä½¿ç”¨çœŸå®ç‰©ç†æ¨¡æ‹Ÿçš„MCTS

å…³é”®ç‰¹æ€§ï¼š
1. ä½¿ç”¨GameInterfaceçš„çœŸå®pymunkç‰©ç†å¼•æ“
2. çœŸå®çš„çƒä½“å¼¹è·³ã€ç¢°æ’ã€åˆå¹¶
3. æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿï¼š
   - åˆå¹¶å¥–åŠ±
   - ä½ç½®ä¼˜åŠ¿å¥–åŠ±ï¼ˆå°æ°´æœåœ¨å¤§æ°´æœä¸Šï¼‰
   - æœªæ¥3æ­¥å¥–åŠ±ç´¯ç§¯
"""

import numpy as np
import math
import copy
from typing import List, Dict, Optional
from Game import FRUIT_RADIUS


class RealPhysicsConfig:
    """Real Physics MCTSé…ç½®"""

    # MCTSå‚æ•°
    C_PUCT = 1.5                    # æ¢ç´¢-åˆ©ç”¨å¹³è¡¡
    NUM_SIMULATIONS = 256           # æ¯æ­¥æ¨¡æ‹Ÿæ¬¡æ•°

    # Rolloutå‚æ•°
    ROLLOUT_STEPS = 2              # æ¯æ¬¡rolloutçš„æ­¥æ•°ï¼ˆå¢åŠ åˆ°5æ­¥ï¼Œçœ‹å¾—æ›´è¿œï¼‰
    FUTURE_STEPS = 5                # è®¡ç®—æœªæ¥å¥–åŠ±çš„æ­¥æ•°

    # å¥–åŠ±æƒé‡
    MERGE_REWARD = 100.0            # åˆå¹¶åŸºç¡€å¥–åŠ±ï¼ˆå¤§å¹…æé«˜ï¼Œé¼“åŠ±åˆå¹¶ï¼‰
    POSITION_REWARD = 10.0          # ä½ç½®ä¼˜åŠ¿å¥–åŠ±ï¼ˆæé«˜ï¼Œé¼“åŠ±åˆ›é€ mergeæœºä¼šï¼‰
    HEIGHT_PENALTY = 2.0            # é«˜åº¦æƒ©ç½šï¼ˆé™ä½ï¼Œè®©åˆå¹¶æ›´é‡è¦ï¼‰
    DEATH_PENALTY = 20000.0           # æ¸¸æˆç»“æŸæƒ©ç½š

    # ç‰©ç†æ¨¡æ‹Ÿå‚æ•°
    PHYSICS_STEPS_PER_ACTION = 160   # æ¯ä¸ªåŠ¨ä½œç‰©ç†æ­¥æ•°ï¼ˆpymunk stepsï¼‰
    WAIT_FRAMES = 10                # ç­‰å¾…ç¨³å®šçš„å¸§æ•°


class RealPhysicsNode:
    """MCTSèŠ‚ç‚¹"""
    __slots__ = ['parent', 'action', 'prior', 'visit_count', 'total_value',
                 'children', 'game_state_snapshot']

    def __init__(self, parent=None, action: int = None, prior: float = 1.0):
        self.parent = parent
        self.action = action
        self.prior = prior
        self.visit_count = 0
        self.total_value = 0.0
        self.children: Dict[int, 'RealPhysicsNode'] = {}
        self.game_state_snapshot = None  # ä¿å­˜æ¸¸æˆçŠ¶æ€å¿«ç…§

    def get_value(self) -> float:
        """è·å–Qå€¼"""
        if self.visit_count == 0:
            return 0.0
        return self.total_value / self.visit_count

    def get_puct(self) -> float:
        """è®¡ç®—PUCTå€¼"""
        if self.parent is None:
            return 0.0

        q = self.get_value()
        u = (RealPhysicsConfig.C_PUCT * self.prior *
             math.sqrt(self.parent.visit_count) / (1 + self.visit_count))
        return q + u

    def select_child(self):
        """é€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹ - ç®€åŒ–ç­–ç•¥ï¼šæœªè®¿é—®ä¼˜å…ˆï¼Œç„¶åé€‰Qå€¼æœ€é«˜"""
        # å¦‚æœæœ‰æœªè®¿é—®çš„èŠ‚ç‚¹ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
        unvisited = [child for child in self.children.values() if child.visit_count == 0]
        if unvisited:
            return np.random.choice(unvisited)

        # æ‰€æœ‰éƒ½è®¿é—®è¿‡äº†ï¼Œé€‰æ‹©Qå€¼æœ€é«˜çš„
        return max(self.children.values(), key=lambda c: c.get_value())

    def expand(self, valid_actions: List[int]):
        """æ‰©å±•èŠ‚ç‚¹"""
        for action in valid_actions:
            if action not in self.children:
                prior = 1.0 / len(valid_actions)
                self.children[action] = RealPhysicsNode(
                    parent=self, action=action, prior=prior
                )

    def update(self, value: float):
        """åå‘ä¼ æ’­æ›´æ–°"""
        self.visit_count += 1
        self.total_value += value

    def best_action(self) -> int:
        """é€‰æ‹©è®¿é—®æ¬¡æ•°æœ€å¤šçš„åŠ¨ä½œ"""
        if not self.children:
            return 8  # é»˜è®¤ä¸­é—´ä½ç½®
        return max(self.children.items(), key=lambda x: x[1].visit_count)[0]


class RealPhysicsMCTS:
    """ä½¿ç”¨çœŸå®ç‰©ç†çš„MCTS"""

    def __init__(self):
        self.root: Optional[RealPhysicsNode] = None
        self.config = RealPhysicsConfig()

    def search(self, env, num_simulations: int) -> int:
        """
        ä¸¤æ­¥å‰ç»æœç´¢ï¼š
        1. æ¯ä¸ªä½ç½®æ‰”ä¸€ä¸ªçƒ
        2. ç‰©ç†æ›´æ–°åå†åœ¨æ¯ä¸ªä½ç½®æ‰”ä¸€ä¸ªçƒ
        3. ç´¯åŠ ä¸¤æ­¥rewardï¼Œé€‰æœ€é«˜çš„

        Args:
            env: GameInterfaceç¯å¢ƒï¼ˆçœŸå®ç‰©ç†ï¼‰
            num_simulations: å¿½ç•¥ï¼ˆä¸éœ€è¦ï¼‰

        Returns:
            æœ€ä½³åŠ¨ä½œ
        """
        # ä¿å­˜å½“å‰ç¯å¢ƒçŠ¶æ€
        original_state = self._save_state(env)

        # æ£€æŸ¥æ¯ä¸ªæ¨ªåæ ‡ä½ç½®çš„æœ€é¡¶éƒ¨æ°´æœ
        current_type = env.game.current_fruit_type
        action_segment_len = env.game.width / 16

        # è®°å½•æ¯ä¸ªactionä½ç½®çš„æœ€ä¸Šæ–¹ï¼ˆyå€¼æœ€å°ï¼‰çš„æ°´æœ
        top_fruits_by_action = {}  # {action: (y, type)}

        for ball, fruit in zip(env.game.balls, env.game.fruits):
            x = ball.body.position.x
            y = ball.body.position.y
            action_pos = int(x / action_segment_len)
            action_pos = max(0, min(15, action_pos))

            # æ›´æ–°è¿™ä¸ªä½ç½®çš„æœ€é¡¶éƒ¨æ°´æœ
            if action_pos not in top_fruits_by_action or y < top_fruits_by_action[action_pos][0]:
                top_fruits_by_action[action_pos] = (y, fruit.type)

        # æ‰¾å‡ºé¡¶éƒ¨æ˜¯ç›¸åŒç±»å‹æ°´æœçš„ä½ç½®
        top_match_actions = []
        for action_pos, (y, ftype) in top_fruits_by_action.items():
            if ftype == current_type:
                top_match_actions.append(action_pos)

        # -------------------------------
        # ğŸ”¥ æ–°å¢ï¼šè¿‡æ»¤å±é™© actionï¼ˆä¸è¦æ‰”åˆ°ä¼šç«‹å³è§¦é¡¶æ­»äº¡çš„åˆ—ï¼‰
        # -------------------------------
        safe_actions = []
        danger_actions = []  # è®°å½•å“ªäº›ä½ç½®ä¸èƒ½é€‰

        for a in range(16):
            # å¦‚æœè¯¥åˆ—æ²¡æœ‰æ°´æœï¼Œåˆ™ä¸€å®šå®‰å…¨
            if a not in top_fruits_by_action:
                safe_actions.append(a)
                continue

            top_y, _ = top_fruits_by_action[a]

            # å¦‚æœé¡¶éƒ¨æ°´æœå·²ç»è¶…è¿‡æ­»äº¡çº¿ï¼ˆinit_yï¼‰ï¼Œåˆ™å±é™©
            if top_y <= env.game.init_y:
                danger_actions.append(a)
            else:
                safe_actions.append(a)

        # å¦‚æœæ‰€æœ‰åœ°æ–¹éƒ½å±é™©ï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œè¿˜æ˜¯å…è®¸æ‰€æœ‰åŠ¨ä½œé¿å…æ­»å¾ªç¯
        if len(safe_actions) == 0:
            actions_to_try = list(range(16))
        else:
            actions_to_try = safe_actions
        # -------------------------------

        # æ ‡è®°èƒ½mergeçš„ä½ç½®ï¼ˆç”¨äºåŠ æˆï¼‰
        merge_actions = set()
        if top_match_actions:
            merge_actions.update(top_match_actions)
        else:
            # æ‰¾æ‰€æœ‰èƒ½mergeçš„ä½ç½®ï¼ˆåŒ…æ‹¬ç›¸é‚»ï¼‰
            for ball, fruit in zip(env.game.balls, env.game.fruits):
                if fruit.type == current_type:
                    fruit_x = ball.body.position.x
                    action_pos = int(fruit_x / action_segment_len)
                    action_pos = max(0, min(15, action_pos))
                    for offset in [-1, 0, 1]:
                        action_idx = action_pos + offset
                        if 0 <= action_idx < 16:
                            merge_actions.add(action_idx)

        # â— ä¸å…è®¸åœ¨å±é™©çš„ä½ç½®åšmerge
        merge_actions = {m for m in merge_actions if m not in danger_actions}

        # è®°å½•æ¯ä¸ªç¬¬ä¸€æ­¥actionçš„æ€»reward
        action_rewards = {}

        # éå†è¦è€ƒè™‘çš„actions
        for action1 in actions_to_try:
            # æ¢å¤åˆ°åˆå§‹çŠ¶æ€
            self._restore_state(env, original_state)

            # è®°å½•åˆå§‹çŠ¶æ€
            score_before = env.game.score
            fruits_before = self._get_fruits_info(env)

            # æ‰§è¡Œç¬¬ä¸€æ­¥
            self._apply_action(env, action1)

            # æ£€æŸ¥ç¬¬ä¸€æ­¥æ˜¯å¦å¯¼è‡´æ¸¸æˆç»“æŸ
            if not env.game.alive:
                # æ¸¸æˆç»“æŸï¼Œå¤§æƒ©ç½š
                action_rewards[action1] = -RealPhysicsConfig.DEATH_PENALTY
                continue

            # è®¡ç®—ç¬¬ä¸€æ­¥reward
            reward1 = self._calculate_reward(env, score_before, fruits_before)

            # è®°å½•ç¬¬ä¸€æ­¥åçš„çŠ¶æ€
            state_after_step1 = self._save_state(env)
            score_after1 = env.game.score
            fruits_after1 = self._get_fruits_info(env)

            # è®¡ç®—æ‰€æœ‰ç¬¬äºŒæ­¥çš„rewardï¼Œæ‰¾æœ€å¤§å€¼
            max_reward2 = float('-inf')

            for action2 in range(16):
                # æ¢å¤åˆ°ç¬¬ä¸€æ­¥åçš„çŠ¶æ€
                self._restore_state(env, state_after_step1)

                # æ‰§è¡Œç¬¬äºŒæ­¥
                self._apply_action(env, action2)

                # æ£€æŸ¥ç¬¬äºŒæ­¥æ˜¯å¦å¯¼è‡´æ¸¸æˆç»“æŸ
                if not env.game.alive:
                    # æ¸¸æˆç»“æŸï¼Œå¤§æƒ©ç½š
                    reward2 = -RealPhysicsConfig.DEATH_PENALTY
                else:
                    # è®¡ç®—ç¬¬äºŒæ­¥reward
                    reward2 = self._calculate_reward(env, score_after1, fruits_after1)

                # æ›´æ–°æœ€å¤§å€¼
                if reward2 > max_reward2:
                    max_reward2 = reward2

            # æ€»reward = ç¬¬ä¸€æ­¥reward + ç¬¬äºŒæ­¥æœ€å¤§reward
            action_rewards[action1] = reward1 + max_reward2

        # æ¢å¤çŠ¶æ€
        self._restore_state(env, original_state)

        # é€‰æ‹©rewardæœ€é«˜çš„action
        # å¦‚æœæœ‰èƒ½mergeçš„ä½ç½®ï¼Œå¿…é¡»ä»è¿™äº›ä½ç½®ä¸­é€‰ï¼›å¦åˆ™ä»æ‰€æœ‰ä½ç½®ä¸­é€‰
        # -------------------------------
        # ğŸ”¥ æœ€ç»ˆåŠ¨ä½œé€‰æ‹©é€»è¾‘ï¼ˆç¡®ä¿ä¸ä¼šé€‰æ‹©å±é™©åŒºåŸŸï¼‰
        # -------------------------------

        safe_actions_set = set(actions_to_try)

        # è¿‡æ»¤ merge_actionsï¼Œåªä¿ç•™å®‰å…¨çš„
        merge_actions = merge_actions & safe_actions_set

        if len(merge_actions) > 0:
            # æœ‰å¯ç”¨çš„ merge è¡Œä¸º â†’ å¼ºåˆ¶ä» merge ä¸­é€‰
            merge_rewards = {a: action_rewards[a] for a in merge_actions}
            best_action = max(merge_rewards.items(), key=lambda x: x[1])[0]

        else:
            # æ²¡æœ‰ merge åŠ¨ä½œ â†’ ä»æ‰€æœ‰å®‰å…¨åŠ¨ä½œä¸­é€‰
            if len(safe_actions_set) > 0:
                safe_rewards = {a: action_rewards[a] for a in safe_actions_set}
                best_action = max(safe_rewards.items(), key=lambda x: x[1])[0]
            else:
                # æç«¯æƒ…å†µï¼šæ‰€æœ‰åŠ¨ä½œéƒ½æ˜¯å±é™©ï¼ˆä¸€èˆ¬ä¸ä¼šå‘ç”Ÿï¼‰
                # â†’ é€€å›ä½¿ç”¨å…¨éƒ¨ 16 åŠ¨ä½œä¸­æœ€é«˜ reward
                best_action = max(action_rewards.items(), key=lambda x: x[1])[0]

        return best_action

    def _simulate(self, env):
        """å•æ¬¡æ¨¡æ‹Ÿ"""
        node = self.root
        path = [node]

        # 1. Selection - é€‰æ‹©åˆ°å¶èŠ‚ç‚¹
        while node.children and not self._is_terminal(env):
            node = node.select_child()
            path.append(node)

            # åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
            if node.action is not None:
                self._apply_action(env, node.action)

        # 2. Expansion - å¦‚æœä¸æ˜¯ç»ˆæ­¢çŠ¶æ€ï¼Œæ‰©å±•ä¸€ä¸ªæ–°èŠ‚ç‚¹
        if not self._is_terminal(env):
            # æ£€æŸ¥æ˜¯å¦æœ‰æœªexpandçš„action
            valid_actions = self._get_valid_actions(env)
            unexpanded = [a for a in valid_actions if a not in node.children]

            if unexpanded:
                # ä½¿ç”¨æ™ºèƒ½ç­–ç•¥é€‰æ‹©è¦expandçš„actionï¼ˆä¼˜å…ˆmergeï¼‰
                action = self._rollout_policy(env)
                # å¦‚æœæ™ºèƒ½é€‰æ‹©çš„actionå·²ç»expandï¼Œéšæœºé€‰ä¸€ä¸ªæœªexpandçš„
                if action not in unexpanded:
                    action = np.random.choice(unexpanded)

                # åªexpandè¿™ä¸€ä¸ªaction
                node.expand([action])
                node = node.children[action]
                path.append(node)
                self._apply_action(env, action)

        # 3. Simulation (Rollout) - ä½¿ç”¨çœŸå®ç‰©ç†æ¨¡æ‹Ÿ
        value = self._rollout(env)

        # 4. Backpropagation - åå‘ä¼ æ’­
        for n in path:
            n.update(value)

    def _rollout(self, env) -> float:
        """
        ä½¿ç”¨çœŸå®ç‰©ç†å¼•æ“çš„rollout

        æ¯æ­¥ï¼š
        1. æ‰§è¡ŒåŠ¨ä½œï¼ˆçœŸå®ç‰©ç†ï¼‰
        2. è®¡ç®—å³æ—¶å¥–åŠ±
        3. ç´¯ç§¯æœªæ¥3æ­¥å¥–åŠ±
        """
        total_reward = 0.0
        rollout_steps = RealPhysicsConfig.ROLLOUT_STEPS

        for step in range(rollout_steps):
            if self._is_terminal(env):
                total_reward -= RealPhysicsConfig.DEATH_PENALTY
                break

            # é€‰æ‹©åŠ¨ä½œï¼ˆç®€å•ç­–ç•¥ï¼šä¸­é—´åå¥½ï¼‰
            
            action = self._rollout_policy(env)

            # è®°å½•æ‰§è¡Œå‰çš„çŠ¶æ€
            score_before = env.game.score
            fruits_before = self._get_fruits_info(env)

            # æ‰§è¡ŒåŠ¨ä½œï¼ˆçœŸå®ç‰©ç†ï¼‰
            self._apply_action(env, action)

            # è®¡ç®—å¥–åŠ±
            reward = self._calculate_reward(env, score_before, fruits_before)

            # æœªæ¥å¥–åŠ±è¡°å‡
            discount = 0.7 ** step
            total_reward += reward * discount

        return total_reward

    def _calculate_reward(self, env, score_before: float,
                         fruits_before: List[dict]) -> float:
        """
        è®¡ç®—æ™ºèƒ½å¥–åŠ±

        å¥–åŠ±æ¥æºï¼š
        1. åˆå¹¶å¥–åŠ±ï¼ˆå¾—åˆ†å¢åŠ ï¼‰
        2. ä½ç½®ä¼˜åŠ¿å¥–åŠ±ï¼ˆå°æ°´æœåœ¨å¤§æ°´æœä¸Šï¼‰
        3. é«˜åº¦æƒ©ç½š
        """
        reward = 0.0

        # 1. åˆå¹¶å¥–åŠ±ï¼ˆå¾—åˆ†å˜åŒ–ï¼‰
        score_delta = env.game.score - score_before
        if score_delta > 0:
            # æœ‰åˆå¹¶å‘ç”Ÿï¼
            reward += score_delta * RealPhysicsConfig.MERGE_REWARD

        # 2. ä½ç½®ä¼˜åŠ¿å¥–åŠ±
        position_bonus = self._evaluate_positions(env, fruits_before)
        reward += position_bonus

        # 3. é«˜åº¦æƒ©ç½š
        height_penalty = self._calculate_height_penalty(env)
        reward -= height_penalty

        return reward

    def _evaluate_positions(self, env, fruits_before: List[dict]) -> float:
        """
        è¯„ä¼°ä½ç½®ä¼˜åŠ¿

        è§„åˆ™ï¼š
        - å°æ°´æœåœ¨ä¸Šä¸€çº§æ°´æœä¸Šé¢ï¼š+0.5å¥–åŠ±
        - ç›¸åŒæ°´æœç›¸é‚»ï¼š+1.0å¥–åŠ±
        """
        bonus = 0.0
        fruits_now = self._get_fruits_info(env)

        # æ£€æŸ¥æ¯ä¸ªæ°´æœ
        for fruit in fruits_now:
            fruit_type = fruit['type']
            fruit_y = fruit['y']

            # æ£€æŸ¥ä¸‹æ–¹æ˜¯å¦æœ‰ä¸Šä¸€çº§æ°´æœ
            for other in fruits_now:
                if other['type'] == fruit_type + 1:  # ä¸Šä¸€çº§
                    # æ£€æŸ¥æ˜¯å¦åœ¨å…¶ä¸Šæ–¹
                    if fruit_y < other['y'] and abs(fruit['x'] - other['x']) < 30:
                        bonus += RealPhysicsConfig.POSITION_REWARD * 0.5

                # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒç±»å‹çš„æ°´æœç›¸é‚»
                if other['type'] == fruit_type and fruit != other:
                    distance = math.sqrt(
                        (fruit['x'] - other['x'])**2 +
                        (fruit['y'] - other['y'])**2
                    )
                    if distance < 60:  # ç›¸é‚»
                        bonus += RealPhysicsConfig.POSITION_REWARD * 1.0

        return bonus

    def _calculate_height_penalty(self, env) -> float:
        """è®¡ç®—é«˜åº¦æƒ©ç½š"""
        balls = env.game.balls
        if not balls:
            return 0.0

        # æ‰¾æœ€é«˜ç‚¹
        min_y = min(b.body.position.y for b in balls)

        # æ¸¸æˆé«˜åº¦
        game_height = env.game.height
        warning_line_y = env.game.init_y  # çº¢çº¿ä½ç½®ï¼ˆ0.15 * heightï¼‰
        # print(f"Warning Line Y: {warning_line_y}, Min Fruit Y: {min_y}")
        # è®¡ç®—å ç”¨æ¯”ä¾‹

        height_ratio = (min_y-warning_line_y ) / warning_line_y
        # print(f"Height Ratio: {height_ratio}")

        # æŒ‡æ•°æƒ©ç½š
        penalty = abs(RealPhysicsConfig.HEIGHT_PENALTY /(height_ratio+0.01))*(-1)
        # print(f"Height Penalty: {penalty}")
        return penalty

    def _get_fruits_info(self, env) -> List[dict]:
        """è·å–æ‰€æœ‰æ°´æœä¿¡æ¯"""
        fruits_info = []
        for ball, fruit in zip(env.game.balls, env.game.fruits):
            fruits_info.append({
                'type': fruit.type,
                'x': ball.body.position.x,
                'y': ball.body.position.y,
            })
        return fruits_info

    def _rollout_policy(self, env) -> int:
        """
        æ™ºèƒ½Rolloutç­–ç•¥ï¼šåªä¼˜å…ˆmergeï¼Œæ— åå¥½
        """
        # è·å–å½“å‰è¦æ‰”çš„æ°´æœç±»å‹
        current_type = env.game.current_fruit_type

        # è·å–åœºä¸Šæ‰€æœ‰æ°´æœä¿¡æ¯
        fruits_info = self._get_fruits_info(env)

        # åˆå§‹åŒ–æƒé‡ï¼ˆæ‰€æœ‰ä½ç½®ç›¸ç­‰ï¼‰
        weights = np.ones(16)

        # å¦‚æœåœºä¸Šæœ‰ç›¸åŒç±»å‹çš„æ°´æœï¼Œå¤§å¹…æé«˜é‚£äº›ä½ç½®çš„æƒé‡
        action_segment_len = env.game.width / 16
        for fruit in fruits_info:
            if fruit['type'] == current_type:
                # æ‰¾åˆ°è¿™ä¸ªæ°´æœå¯¹åº”çš„actionä½ç½®
                fruit_x = fruit['x']
                best_action = int(fruit_x / action_segment_len)
                best_action = max(0, min(15, best_action))  # é™åˆ¶èŒƒå›´

                # å¤§å¹…æé«˜è¯¥ä½ç½®åŠç›¸é‚»ä½ç½®çš„æƒé‡ï¼ˆé¼“åŠ±mergeï¼‰
                for offset in [-1, 0, 1]:
                    action_idx = best_action + offset
                    if 0 <= action_idx < 16:
                        weights[action_idx] *= 100.0  # æé«˜100å€æƒé‡ï¼

        # å½’ä¸€åŒ–
        weights = weights / weights.sum()

        action = np.random.choice(16, p=weights)
        
        return action

    def _apply_action(self, env, action: int):
        """åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼ˆçœŸå®ç‰©ç†ï¼‰"""
        # æ‰§è¡ŒåŠ¨ä½œ
        env.next(action)
        if not env.game.alive:
            return
        # ç­‰å¾…ç‰©ç†ç¨³å®š
        for _ in range(RealPhysicsConfig.WAIT_FRAMES):
            env.game.space.step(1/60.0)

    def _get_valid_actions(self, env) -> List[int]:
        """è·å–æœ‰æ•ˆåŠ¨ä½œ"""
        # æ‰€æœ‰16åˆ—éƒ½å¯ç”¨ï¼ˆçœŸå®ç‰©ç†ä¼šå¤„ç†ç¢°æ’ï¼‰
        return list(range(16))

    def _is_terminal(self, env) -> bool:
        """æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢"""
        return not env.game.alive

    def _save_state(self, env) -> dict:
        """ä¿å­˜æ¸¸æˆçŠ¶æ€"""
        game = env.game
        state = {
            'score': game.score,
            'alive': game.alive,
            'current_fruit_type': game.current_fruit_type,
            'largest_fruit_type': game.largest_fruit_type,
            'balls': []
        }

        # ä¿å­˜æ‰€æœ‰æ°´æœä¿¡æ¯
        for ball, fruit in zip(game.balls, game.fruits):
            ball_info = {
                'type': fruit.type,
                'x': ball.body.position.x,
                'y': ball.body.position.y,
                'vx': ball.body.velocity.x,
                'vy': ball.body.velocity.y,
                'angle': ball.body.angle,
                'angular_velocity': ball.body.angular_velocity,
            }
            state['balls'].append(ball_info)

        return state

    def _restore_state(self, env, state: dict):
        """æ¢å¤æ¸¸æˆçŠ¶æ€"""
        game = env.game

        # æ¸…é™¤æ‰€æœ‰æ°´æœ
        for ball in list(game.balls):
            game.space.remove(ball, ball.body)
        game.balls.clear()
        game.fruits.clear()

        # æ¢å¤åŸºæœ¬çŠ¶æ€
        game.score = state['score']
        game.alive = state['alive']
        game.current_fruit_type = state['current_fruit_type']
        game.largest_fruit_type = state['largest_fruit_type']

        # æ¢å¤æ°´æœ
        from Game import Fruit
        for ball_info in state['balls']:
            # åˆ›å»ºball
            ball = game.create_ball(
                game.space,
                ball_info['x'],
                ball_info['y'],
                radius=FRUIT_RADIUS[ball_info['type']],
                type=ball_info['type']
            )
            ball.body.velocity = (ball_info['vx'], ball_info['vy'])
            ball.body.angle = ball_info['angle']
            ball.body.angular_velocity = ball_info['angular_velocity']

            # åˆ›å»ºå¯¹åº”çš„fruit
            fruit = Fruit(ball_info['type'], ball_info['x'], ball_info['y'])

            game.balls.append(ball)
            game.fruits.append(fruit)


class RealPhysicsMCTSAgent:
    """Real Physics MCTS Agent"""

    def __init__(self, num_simulations: int = 100):
        self.mcts = RealPhysicsMCTS()
        self.num_simulations = num_simulations

    def predict(self, env) -> np.ndarray:
        """é¢„æµ‹åŠ¨ä½œ"""
        action = self.mcts.search(env, self.num_simulations)
        return np.array([action])

    def sample(self, env) -> np.ndarray:
        """é‡‡æ ·åŠ¨ä½œ"""
        return self.predict(env)


if __name__ == "__main__":
    print("="*70)
    print("Real Physics MCTS")
    print("="*70)
    print("\né…ç½®:")
    print(f"  æ¨¡æ‹Ÿæ¬¡æ•°: {RealPhysicsConfig.NUM_SIMULATIONS}")
    print(f"  Rolloutæ­¥æ•°: {RealPhysicsConfig.ROLLOUT_STEPS}")
    print(f"  æœªæ¥æ­¥æ•°: {RealPhysicsConfig.FUTURE_STEPS}")
    print(f"\nå¥–åŠ±:")
    print(f"  åˆå¹¶å¥–åŠ±: {RealPhysicsConfig.MERGE_REWARD}")
    print(f"  ä½ç½®å¥–åŠ±: {RealPhysicsConfig.POSITION_REWARD}")
    print(f"  é«˜åº¦æƒ©ç½š: {RealPhysicsConfig.HEIGHT_PENALTY}")
    print(f"  æ­»äº¡æƒ©ç½š: {RealPhysicsConfig.DEATH_PENALTY}")
    print("="*70)
