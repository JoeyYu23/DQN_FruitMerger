"""
5000è½®DQNè®­ç»ƒ - å¸¦å®æ—¶è¿›åº¦æ˜¾ç¤º
"""
import os
import json
import numpy as np
import paddle
from datetime import datetime, timedelta
import time
from DQN import (
    Agent, build_model, ReplayMemory, MEMORY_SIZE, MEMORY_WARMUP_SIZE,
    BATCH_SIZE, LEARNING_RATE, GAMMA, set_global_seed
)
from GameInterface import GameInterface

# ==================== è®­ç»ƒé…ç½® ====================
TRAINING_SEED = 42
MAX_EPISODES = 5000
EVAL_INTERVAL = 100  # æ¯100è½®è¯„ä¼°ä¸€æ¬¡ï¼ˆ5000è½®å¤ªå¤šï¼Œ25è½®å¤ªé¢‘ç¹ï¼‰
EVAL_EPISODES = 10
LOG_FILE = "training_metrics_5000.json"
MODEL_SAVE_DIR = "weights"
CHECKPOINT_INTERVAL = 500  # æ¯500è½®ä¿å­˜checkpoint

# éªŒè¯é›†å’Œæµ‹è¯•é›†çš„å›ºå®šç§å­
VAL_SEEDS = list(range(10000, 10000 + EVAL_EPISODES))
TEST_SEEDS = list(range(20000, 20000 + EVAL_EPISODES))


class ProgressTracker:
    """è®­ç»ƒè¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self, total_episodes):
        self.total_episodes = total_episodes
        self.start_time = None
        self.episode_times = []
        self.last_print_time = time.time()

    def start(self):
        """å¼€å§‹è®­ç»ƒ"""
        self.start_time = time.time()

    def update(self, episode, metrics):
        """æ›´æ–°è¿›åº¦"""
        current_time = time.time()

        # è®°å½•episodeæ—¶é—´
        if len(self.episode_times) > 0:
            episode_time = current_time - self.last_update_time
            self.episode_times.append(episode_time)
            # åªä¿ç•™æœ€è¿‘100ä¸ªepisodeçš„æ—¶é—´
            if len(self.episode_times) > 100:
                self.episode_times.pop(0)

        self.last_update_time = current_time

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        elapsed = current_time - self.start_time
        progress = episode / self.total_episodes

        # ä¼°è®¡å‰©ä½™æ—¶é—´
        if len(self.episode_times) > 0:
            avg_episode_time = np.mean(self.episode_times)
            remaining_episodes = self.total_episodes - episode
            eta_seconds = avg_episode_time * remaining_episodes
            eta = timedelta(seconds=int(eta_seconds))
        else:
            eta = "è®¡ç®—ä¸­..."

        # åªåœ¨æ»¡è¶³æ¡ä»¶æ—¶æ‰“å°ï¼ˆé¿å…åˆ·å±ï¼‰
        should_print = (
            episode % 10 == 0 or  # æ¯10ä¸ªepisode
            current_time - self.last_print_time > 5  # æˆ–æ¯5ç§’
        )

        if should_print:
            self.print_progress(episode, progress, elapsed, eta, metrics)
            self.last_print_time = current_time

    def print_progress(self, episode, progress, elapsed, eta, metrics):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        # è¿›åº¦æ¡
        bar_length = 40
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)

        # æ ¼å¼åŒ–æ—¶é—´
        elapsed_str = str(timedelta(seconds=int(elapsed)))

        # æ¸…é™¤å½“å‰è¡Œå¹¶æ‰“å°
        print(f'\r', end='')  # å›åˆ°è¡Œé¦–
        print(
            f"[{bar}] {progress*100:5.1f}% | "
            f"Ep {episode:4d}/{self.total_episodes} | "
            f"â±ï¸ {elapsed_str} | "
            f"â³ ETA {eta} | "
            f"Score {metrics['score']:3.0f} | "
            f"Loss {metrics['mean_loss']:6.1f} | "
            f"Îµ {metrics['epsilon']:.4f}",
            end='', flush=True
        )

    def print_eval(self, episode, train_metrics, val_metrics):
        """æ‰“å°è¯„ä¼°ä¿¡æ¯"""
        print()  # æ¢è¡Œ
        print("=" * 100)
        print(f"ğŸ“Š Episode {episode}/{self.total_episodes} - è¯„ä¼°ç»“æœ")
        print("-" * 100)

        elapsed = time.time() - self.start_time
        elapsed_str = str(timedelta(seconds=int(elapsed)))
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        print(f"â° å½“å‰æ—¶é—´: {current_time} | å·²è®­ç»ƒæ—¶é—´: {elapsed_str}")
        print()

        print(f"è®­ç»ƒé›† (æœ¬è½®):")
        print(f"  åˆ†æ•°: {train_metrics['score']:6.1f} | "
              f"å¥–åŠ±: {train_metrics['reward']:8.1f} | "
              f"Loss: {train_metrics['mean_loss']:8.2f} | "
              f"æ­¥æ•°: {train_metrics['steps']:4d}")

        print(f"éªŒè¯é›† (10å±€å¹³å‡):")
        print(f"  åˆ†æ•°: {val_metrics['mean_score']:6.1f} Â± {val_metrics['std_score']:5.1f} | "
              f"å¥–åŠ±: {val_metrics['mean_reward']:8.1f} Â± {val_metrics['std_reward']:5.1f} | "
              f"æœ€é«˜: {val_metrics['max_score']:3d} | "
              f"æœ€ä½: {val_metrics['min_score']:3d}")

        print(f"æ¢ç´¢ç‡: Îµ = {train_metrics['epsilon']:.6f}")
        print("=" * 100)
        print()


def evaluate_agent(env, agent, seeds, num_episodes=None):
    """è¯„ä¼°agent"""
    if num_episodes is None:
        num_episodes = len(seeds)

    scores = []
    rewards = []

    for i in range(num_episodes):
        seed = seeds[i]
        env.reset(seed=seed)

        action = np.random.randint(0, env.action_num)
        feature, _, alive = env.next(action)

        episode_reward = 0
        while alive:
            action = agent.predict(feature)
            feature, reward, alive = env.next(action)
            episode_reward += np.sum(reward)

        scores.append(env.game.score)
        rewards.append(episode_reward)

    return {
        'mean_score': float(np.mean(scores)),
        'std_score': float(np.std(scores)),
        'mean_reward': float(np.mean(rewards)),
        'std_reward': float(np.std(rewards)),
        'max_score': int(np.max(scores)),
        'min_score': int(np.min(scores)),
    }


def run_training_episode(env, agent, memory):
    """è¿è¡Œä¸€ä¸ªè®­ç»ƒepisode"""
    env.reset()

    step = 0
    rewards_sum = 0
    losses = []

    action = np.random.randint(0, env.action_num)
    feature, _, alive = env.next(action)

    while alive:
        step += 1

        action = agent.sample(feature)
        next_feature, reward, alive = env.next(action)

        reward = reward if alive else -1000
        memory.append((feature, action, reward, next_feature, alive))

        if len(memory) >= MEMORY_WARMUP_SIZE and agent.global_step % 1 == 0:
            batch = memory.sample(BATCH_SIZE)
            loss = agent.learn(*batch)
            losses.append(loss)

        rewards_sum += np.sum(reward)
        feature = next_feature
        agent.global_step += 1

    return {
        'score': int(env.game.score),
        'reward': float(rewards_sum),
        'steps': int(step),
        'mean_loss': float(np.mean(losses)) if losses else 0.0,
        'epsilon': float(agent.e_greed)
    }


def save_checkpoint(agent, episode, filename):
    """ä¿å­˜è®­ç»ƒcheckpoint"""
    checkpoint = {
        'episode': episode,
        'model_state_dict': agent.policy_net.state_dict(),
        'optimizer_state_dict': agent.optimizer.state_dict(),
        'epsilon': agent.e_greed,
        'global_step': agent.global_step,
    }
    paddle.save(checkpoint, filename)


def main():
    print("=" * 100)
    print("ğŸš€ DQNè®­ç»ƒ - 5000è½®å®Œæ•´è®­ç»ƒ")
    print("=" * 100)

    # è®¾ç½®éšæœºç§å­
    set_global_seed(TRAINING_SEED)

    # åˆå§‹åŒ–ç¯å¢ƒå’Œagent
    env = GameInterface()
    feature_dim = env.FEATURE_MAP_HEIGHT * env.FEATURE_MAP_WIDTH * 2
    action_dim = env.ACTION_NUM

    memory = ReplayMemory(MEMORY_SIZE)
    agent = Agent(
        build_model,
        feature_dim,
        action_dim,
        e_greed=0.5,
        e_greed_decrement=1e-6
    )

    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  ç‰¹å¾ç»´åº¦: {feature_dim}")
    print(f"  åŠ¨ä½œç©ºé—´: {action_dim}")
    print(f"  è®­ç»ƒç§å­: {TRAINING_SEED}")
    print(f"  æœ€å¤§è®­ç»ƒè½®æ•°: {MAX_EPISODES}")
    print(f"  è¯„ä¼°é—´éš”: æ¯{EVAL_INTERVAL}è½®")
    print(f"  Checkpointé—´éš”: æ¯{CHECKPOINT_INTERVAL}è½®")
    print()

    # é¢„çƒ­ç»éªŒæ± 
    print(f"ğŸ”¥ é¢„çƒ­ç»éªŒæ±  (ç›®æ ‡: {MEMORY_WARMUP_SIZE})...")
    warmup_start = time.time()
    warmup_count = 0

    while len(memory) < MEMORY_WARMUP_SIZE:
        run_training_episode(env, agent, memory)
        warmup_count += 1
        if warmup_count % 100 == 0:
            print(f"  é¢„çƒ­è¿›åº¦: {len(memory):5d}/{MEMORY_WARMUP_SIZE} ({len(memory)/MEMORY_WARMUP_SIZE*100:.1f}%)", end='\r')

    warmup_time = time.time() - warmup_start
    print(f"\nâœ… ç»éªŒæ± é¢„çƒ­å®Œæˆï¼Œå…± {len(memory)} æ¡ç»éªŒ (è€—æ—¶: {warmup_time:.1f}ç§’)\n")

    # è®­ç»ƒæ•°æ®è®°å½•
    training_log = {
        'config': {
            'seed': TRAINING_SEED,
            'max_episodes': MAX_EPISODES,
            'memory_size': MEMORY_SIZE,
            'batch_size': BATCH_SIZE,
            'learning_rate': LEARNING_RATE,
            'gamma': GAMMA,
            'eval_interval': EVAL_INTERVAL,
            'checkpoint_interval': CHECKPOINT_INTERVAL,
        },
        'training': [],
        'validation': [],
        'test': []
    }

    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
    progress = ProgressTracker(MAX_EPISODES)
    progress.start()

    best_val_score = 0

    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("-" * 100)
    print()

    for episode in range(MAX_EPISODES + 1):
        # è®­ç»ƒä¸€ä¸ªepisode
        episode_metrics = run_training_episode(env, agent, memory)

        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        training_log['training'].append({
            'episode': episode,
            **episode_metrics
        })

        # æ›´æ–°è¿›åº¦
        progress.update(episode, episode_metrics)

        # å®šæœŸè¯„ä¼°
        if episode % EVAL_INTERVAL == 0:
            # éªŒè¯é›†è¯„ä¼°
            val_metrics = evaluate_agent(env, agent, VAL_SEEDS, EVAL_EPISODES)
            training_log['validation'].append({
                'episode': episode,
                **val_metrics
            })

            # æ‰“å°è¯„ä¼°ä¿¡æ¯
            progress.print_eval(episode, episode_metrics, val_metrics)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['mean_score'] > best_val_score:
                best_val_score = val_metrics['mean_score']
                best_model_path = os.path.join(MODEL_SAVE_DIR, "best_model.pdparams")
                paddle.save(agent.policy_net.state_dict(), best_model_path)
                print(f"ğŸ† æ–°æœ€ä½³éªŒè¯åˆ†æ•°: {best_val_score:.1f} - æ¨¡å‹å·²ä¿å­˜åˆ° {best_model_path}\n")

        # å®šæœŸä¿å­˜checkpoint
        if episode > 0 and episode % CHECKPOINT_INTERVAL == 0:
            checkpoint_path = os.path.join(MODEL_SAVE_DIR, f"checkpoint_ep{episode}.pdparams")
            save_checkpoint(agent, episode, checkpoint_path)
            print(f"\nğŸ’¾ Checkpointå·²ä¿å­˜: {checkpoint_path}\n")

    print("\n")
    print("=" * 100)

    # æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°
    print("ğŸ“ æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°...")
    test_metrics = evaluate_agent(env, agent, TEST_SEEDS, len(TEST_SEEDS))
    training_log['test'].append({
        'episode': MAX_EPISODES,
        **test_metrics
    })

    print(f"\næµ‹è¯•é›†ç»“æœ:")
    print(f"  åˆ†æ•°: {test_metrics['mean_score']:.1f} Â± {test_metrics['std_score']:.1f}")
    print(f"  å¥–åŠ±: {test_metrics['mean_reward']:.1f} Â± {test_metrics['std_reward']:.1f}")
    print(f"  æœ€é«˜: {test_metrics['max_score']} | æœ€ä½: {test_metrics['min_score']}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "final_5000.pdparams"
    paddle.save(agent.policy_net.state_dict(), final_model_path)
    print(f"\nâœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    with open(LOG_FILE, 'w') as f:
        json.dump(training_log, f, indent=2)
    print(f"âœ… è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {LOG_FILE}")

    # è®­ç»ƒæ€»ç»“
    total_time = time.time() - progress.start_time
    total_time_str = str(timedelta(seconds=int(total_time)))

    print("\n" + "=" * 100)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 100)
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time_str}")
    print(f"å¹³å‡æ¯è½®æ—¶é—´: {total_time/MAX_EPISODES:.2f}ç§’")
    print(f"æœ€ä½³éªŒè¯åˆ†æ•°: {best_val_score:.1f}")
    print(f"æœ€ç»ˆæµ‹è¯•åˆ†æ•°: {test_metrics['mean_score']:.1f}")
    print("=" * 100)


if __name__ == "__main__":
    main()
